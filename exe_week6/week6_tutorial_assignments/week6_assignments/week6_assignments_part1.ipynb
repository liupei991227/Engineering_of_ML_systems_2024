{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week6 Assignments (part 1)\n",
    "\n",
    "**Please do all the assignments for this week using the \"mlops_eng3\" Conda environment.** (You can create it by following `week6_tutorial.ipynb` in the tutorial directory.)\n",
    "\n",
    "You'll gain some hands-on experience with monitoring your model using Prometheus and Evidently in this week's assignments. This week's assignments consist of three parts. The first part (this notebook) is about configuring Prometheus alert rules for your inference service. In the [second part](./week6_assignments_part2.ipynb), you'll use Evidently to monitor your model's performance, including different phases needed in monitoring model performance, such as collecting the ground truth and using Evidently to calculate the model performance metrics. In the [final part](./week6_assignments_part3.ipynb), you'll build a KKP pipeline to unify the phases required to monitor model performance. \n",
    "\n",
    "**Guidelines for submitting assignments**:\n",
    "- In the first part, you'll need to write some configurations in a YAML file (`manifests/prometheus-config-patch.yaml`), so please include the YAML file in your submission.  \n",
    "- For every remaining assignment, a code skeleton is provided. Please put your solutions between the `### START CODE HERE` and `### END CODE HERE` code comments. Please **do not change any code other than those between the `### START CODE HERE` and `### END CODE HERE` comments**.\n",
    "- Some assignments also require you to capture screenshots. Please put your screenshots in a PDF. \n",
    "- As for submission, please include the following files:\n",
    "    - `prometheus-config-patch.yaml` (You should find it in the \"manifests\" directory)\n",
    "    - `week6_assignments_part2.ipynb` and `week6_assignments_part3.ipynb` (You don't need to return `week6_assignments_part1.ipynb`)\n",
    "    - `pipeline.yaml` (this file will be generated when you complete the third part of the assignments)\n",
    "    - The PDF containing your screenshots\n",
    "\n",
    "***Important!*** When submitting the files, please **do not** change the file names or put any of them in any sub-folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.config import FEATURE_STORE_DIR_NAME\n",
    "from utils.utils import get_model_info, train, send_requests\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "assert lightgbm.__version__ == \"3.3.5\", \"Incorrect version of lightgbm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation before starting the assignments\n",
    "Let's begin by training a model and deploying it to KServe. This model is trained using the house price data that we used in Week2 assignments. The task of the model is to predict the price of a house given the house's information such as building year and living area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw training data can be found from \"raw_data/reference/train\" directory. The training data has been feature-engineered using the `etl` function from the second week and are split into a feature file (0_0_X.parquet) and a target file (0_0_y.csv) in the \"feature_store_quarterly\" directory. (The name of the feature store directory comes from the fact that we'll save features and targets in a quarterly basis in it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no model with a 'stage-Production' tag, start training a model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Week6LgbmHousePrice' already exists. Creating a new version of this model...\n",
      "2024/12/10 15:54:04 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Week6LgbmHousePrice, version 2\n",
      "Created version '2' of model 'Week6LgbmHousePrice'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model version is: 2, its S3 URI is: s3://mlflow/11/66d2b291052147e08ad5b2feb32dd4f2/artifacts/lgbm-house\n"
     ]
    }
   ],
   "source": [
    "WORKING_DIR = Path.cwd()\n",
    "# Prepare training data\n",
    "train_x = pd.read_parquet(WORKING_DIR / FEATURE_STORE_DIR_NAME / \"0_0_X.parquet\")\n",
    "train_y = pd.read_csv(WORKING_DIR / FEATURE_STORE_DIR_NAME / \"0_0_y.csv\")\n",
    "\n",
    "# Model hyperparameters (hyperparameter optimization was performed)\n",
    "params = {\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"learning_rate\": 0.075,\n",
    "    \"max_depth\": 50,\n",
    "    \"min_child_samples\": 5,\n",
    "    \"min_split_gain\": 20.0,\n",
    "    \"n_estimators\": 1000,\n",
    "    \"num_leaves\": 100,\n",
    "    \"reg_lambda\": 50.0,\n",
    "    \"subsample\": 0.1,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "model_info = get_model_info()\n",
    "if model_info is None:\n",
    "    print(\"There is no model with a 'stage-Production' tag, start training a model\")\n",
    "    model_info = train(train_x, train_y, params)\n",
    "model_version = model_info.model_version\n",
    "model_s3_uri = model_info.model_s3_uri\n",
    "print(f\"The model version is: {model_version}, its S3 URI is: {model_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code cell above, you should see the model you just trained has a tag \"stage\" and the value is \"Production\" at [http://mlflow-server.local](http://mlflow-server.local). \n",
    "\n",
    "<img src=\"./images/mlflow-prod-model.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll also see the S3 URI of your uploaded model printed . Let't then deploy the model to KServe. Before running the next cell, replace the `storageUri` in [manifests/house-price.yaml](./manifests/house-price.yaml) with your own S3 URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/house-price configured\n"
     ]
    }
   ],
   "source": [
    "# Deploy an inference service named \"house-price\"\n",
    "!kubectl apply -f manifests/house-price.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/house-price created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION           AGE\n",
      "house-price   http://house-price.kserve-inference.example.com   True           100                              house-price-predictor-00003   3d2h\n"
     ]
    }
   ],
   "source": [
    "# Check if the \"house-price\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc house-price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                   AGE\n",
    "house-price   http://house-price.kserve-inference.example.com   True           100                              house-price-predictor-default-00001   55s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                      READY   STATUS        RESTARTS   AGE\n",
      "house-price-predictor-00002-deployment-54df65f6d7-fpclm   2/2     Terminating   0          3d2h\n",
      "house-price-predictor-00003-deployment-64b978ff9c-lfmvm   2/2     Running       0          48s\n"
     ]
    }
   ],
   "source": [
    "# Then make sure there's a running pod for the \"house-price\" inference service\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=house-price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "house-price-predictor-default-00001-deployment-748bc8bc67-r7p46   2/2     Running   0          68s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Monitoring 4xx responses of your inference service (2 points)\n",
    "In this assignment, your task is to add a Prometheus alerting rule so that Prometheus will trigger an alert when your inference service gives too many client error responses (i.e., responses whose HTTP status code is 4xx). \n",
    "\n",
    "(If HTTP status codes are new to you, more information can be found [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes).)\n",
    "\n",
    "You need to add an alerting rule to [manifests/prometheus-config-patch.yaml](./manifests/prometheus-config-patch.yaml) that **immediately** triggers an alert when your \"house-price\" inference service running in the \"kserve-inference\" namespace gives more than **ten** 4xx error responses in the **past one minute**. \n",
    "\n",
    "Please add your alerting rule between the comments \"### START ALERTING RULE\" and \"### END ALERTING RULE\". Please **do not change any text other than the one between the \"### START ALERTING RULE\" and \"### END ALERTING RULE\" comments**. Please **include the file \"prometheus-config-patch.yaml\" in your submission.**\n",
    "\n",
    "Hints:\n",
    "- An essential part of configuring an alerting rule is to decide which PromQL query to use. It may be easier if you first test your query in the Prometheus UI [http://prometheus-server.local](http://prometheus-server.local) before writing your alerting rule to the configuration file. You can use the following code cell to send some invalid requests to your inference service and then go to the Prometheus UI to test if your query can retrieve a reasonable value. \n",
    "- You may find the `revision_app_request_count` metric useful (introduced in the tutorial). You can then use the labels `namespace_name`, `isvc_name`, `response_code_class` to only include the responses you want to monitor. \n",
    "- You may also find the PromQL function [increase()](https://prometheus.io/docs/prometheus/latest/querying/functions/#increase) useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 requests have been sent\n",
      "2 requests have been sent\n",
      "3 requests have been sent\n",
      "4 requests have been sent\n",
      "5 requests have been sent\n",
      "6 requests have been sent\n",
      "7 requests have been sent\n",
      "8 requests have been sent\n",
      "9 requests have been sent\n",
      "10 requests have been sent\n",
      "11 requests have been sent\n",
      "12 requests have been sent\n",
      "13 requests have been sent\n",
      "14 requests have been sent\n",
      "15 requests have been sent\n",
      "16 requests have been sent\n",
      "17 requests have been sent\n",
      "18 requests have been sent\n",
      "19 requests have been sent\n",
      "20 requests have been sent\n"
     ]
    }
   ],
   "source": [
    "# Let's pretend your downstream application is somehow broken and starts sending invalid requests to the inference service. \n",
    "# Inside each request, the input data is in a wrong format so the inference service will return responses with\n",
    "# 422 (unprocessable entity) HTTP status code\n",
    "send_requests(model_name=\"house-price\", input=[None for _ in range(16)], count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/prometheus-server-conf patched (no change)\n"
     ]
    }
   ],
   "source": [
    "# Update the Prometheus configuration by patching your alerting rule to the ConfigMap consumed by the Prometheus pod\n",
    "!kubectl -n monitoring patch configmap prometheus-server-conf --patch-file manifests/prometheus-config-patch.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"prometheus-deployment-7df47656d7-4qtws\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete the old Prometheus pod so a new one that consumes the updated ConfigMap will be recreated automatically\n",
    "!kubectl -n monitoring delete pod -l app=prometheus-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                     READY   STATUS    RESTARTS   AGE\n",
      "prometheus-deployment-7df47656d7-w5gnm   1/1     Running   0          4s\n"
     ]
    }
   ],
   "source": [
    "# Check if the new Prometheus pod is ready\n",
    "!kubectl -n monitoring get pod -l app=prometheus-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                     READY   STATUS    RESTARTS   AGE\n",
    "prometheus-deployment-7b898cb9d8-g8wd2   1/1     Running   0          6s\n",
    "```\n",
    "The \"AGE\" should be relatively small since this pod should be created after you deleted the old one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 requests have been sent\n",
      "2 requests have been sent\n",
      "3 requests have been sent\n",
      "4 requests have been sent\n",
      "5 requests have been sent\n",
      "6 requests have been sent\n",
      "7 requests have been sent\n",
      "8 requests have been sent\n",
      "9 requests have been sent\n",
      "10 requests have been sent\n",
      "11 requests have been sent\n",
      "12 requests have been sent\n",
      "13 requests have been sent\n",
      "14 requests have been sent\n",
      "15 requests have been sent\n",
      "16 requests have been sent\n",
      "17 requests have been sent\n",
      "18 requests have been sent\n",
      "19 requests have been sent\n",
      "20 requests have been sent\n",
      "21 requests have been sent\n",
      "22 requests have been sent\n",
      "23 requests have been sent\n",
      "24 requests have been sent\n",
      "25 requests have been sent\n",
      "26 requests have been sent\n",
      "27 requests have been sent\n",
      "28 requests have been sent\n",
      "29 requests have been sent\n",
      "30 requests have been sent\n",
      "31 requests have been sent\n",
      "32 requests have been sent\n",
      "33 requests have been sent\n",
      "34 requests have been sent\n",
      "35 requests have been sent\n",
      "36 requests have been sent\n",
      "37 requests have been sent\n",
      "38 requests have been sent\n",
      "39 requests have been sent\n",
      "40 requests have been sent\n"
     ]
    }
   ],
   "source": [
    "# Send invalid requests again and you should see an alert of too many 4xx responses has been triggered\n",
    "send_requests(model_name=\"house-price\", input=[None for _ in range(16)], count=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can go to [http://prometheus-server.local/alerts](http://prometheus-server.local/alerts) and see if the alert of too many 4xx responses is triggered.\n",
    "\n",
    "### Screenshots for Assignment 1\n",
    "Please screenshot the triggered alert and put it in your PDF file. Extend the alert field so your PromQL query is shown in the screenshot. Note the example is about another alert, it's just used to show you what should be included in the screenshot. \n",
    "\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/alert-example.png\" width=1000/>\n",
    "</details>\n",
    "\n",
    "You can go to the [second part](./week6_assignments_part2.ipynb) of the assignments. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_eng3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
