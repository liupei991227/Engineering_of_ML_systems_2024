{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 assignments\n",
    "**Please do the assignments using the `mlops_eng` environment.**\n",
    "\n",
    "In this week's assignments, you will train a LightGBM regression model to predict public bike sharing demand given attributes like datetime and weather conditions. The dataset (\"bike_sharing_demand.csv\", located under the same directory as this notebook) used in this week's assignments is a preprocessed version of [this kaggle dataset](https://www.kaggle.com/competitions/bike-sharing-demand/overview). You'll learn more about data preprocessing next week. Additionally, you'll use MLflow to track the model training and Deepchecks to evaluate the trained model. \n",
    "\n",
    "**Guidelines for submitting assignments**:\n",
    "- For each assignment, a code skeleton is provided. Please put your solutions between the `### START CODE HERE` and `### END CODE HERE` code comments. Please **do not change any code other than those between the `### START CODE HERE` and `### END CODE HERE` comments**. Otherwise your notebook may not pass the tests used in grading.\n",
    "- Some assignments also require you to capture screenshots in order to earn points. Please put all your screenshots into a single PDF file. For each screenshot, please clearly indicate which assignment it corresponds to in your PDF file.\n",
    "- Please return this notebook and the PDF file containing your screenshots as your submission. \n",
    "\n",
    "In case type hints are new to you, you'll see something below in some code skeletons:\n",
    "```python\n",
    "def greeting(name: str) -> str:\n",
    "    return 'Hello ' + name  \n",
    "```\n",
    "The annotation `name: str` means the parameter `name` is expected to be of type `str` and `-> str` means the type of the returned value is also `str`. These type hints help you understand the function's input requirements and expected output in the assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eebf25c8ffef17e68d7ec793e65956af",
     "grade": false,
     "grade_id": "cell-d7a23654690c7334",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 0: Set up the course environment (4 points)\n",
    "You can earn 4 points for successfully setting up the course environment. To do so, simply assign \"yes\" to the `setup_ok` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43dc03cc9f9f5d5067d098bb736c1759",
     "grade": false,
     "grade_id": "cell-184812ca0b76897b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: setup_ok = ___\n",
    "### START CODE HERE\n",
    "setup_ok = \"yes\"\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ede1ccb4885071dcd37f0c1a6311343",
     "grade": true,
     "grade_id": "cell-c1317ce38a33a57b",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert setup_ok.lower() == \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1245d1477987bec486a49817ac227441",
     "grade": false,
     "grade_id": "cell-d87da24147c10832",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "import mlflow\n",
    "from deepchecks import SuiteResult, CheckResult\n",
    "from deepchecks.tabular import Suite\n",
    "from deepchecks.tabular.checks import TrainTestPerformance, ModelInferenceTime, MultiModelPerformanceReport\n",
    "from deepchecks.tabular import Dataset\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "import boto3\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "PARENT_DIR = Path(\"\").parent.absolute()\n",
    "\n",
    "# Suppress boto3 logging\n",
    "boto3.set_stream_logger(name='botocore.credentials', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a07b16cbaad3d8170aeef48de545699",
     "grade": false,
     "grade_id": "cell-79b3a8091529df9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is just for the grading purpose\n",
    "def is_being_graded():\n",
    "    \"\"\"\n",
    "    Returns True if the notebook is being executed by the auto-grading tool.\n",
    "    \"\"\"\n",
    "    env = os.environ.get(\"NBGRADER_EXECUTION\")\n",
    "    return env == \"autograde\" or env == \"validate\"\n",
    "\n",
    "\n",
    "# Suppress loggings and warnings when grading the notebook\n",
    "if is_being_graded():\n",
    "    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "    for logger in loggers:\n",
    "        logger.setLevel(logging.ERROR)\n",
    "    mlflow.utils.logging_utils.disable_logging()\n",
    "    warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88ebb09bba435d56860e2a1c06fac249",
     "grade": false,
     "grade_id": "cell-2f0a1b10a2d62f06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def delete_file_if_existing(filename: str):\n",
    "    \"\"\"\n",
    "    Delete a file if it's existing\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Delete the existing {filename}\")\n",
    "        os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f27748f8d6e0014105024d721c1662b",
     "grade": false,
     "grade_id": "cell-c13a0ca740e6a5e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 1: Download and split the dataset (2 points)\n",
    "Please note that the dataset (bike_sharing_demand.csv) contains data collected from two years so you'll find duplicated data points for most of the timestamps (hour-day-month) if you explore the dataset. \n",
    "\n",
    "### 1a) Load the data\n",
    "First, implement the `pull_data` function that loads a CSV as a Pandas DataFrame from a given location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf4c36ef6e35b1fcbdaab50e29a96ec4",
     "grade": false,
     "grade_id": "cell-416028b27a69c2cb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pull_data(dataset_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download the data set from a given path\n",
    "    Args: \n",
    "        dataset_path (Path): Path of the CSV \n",
    "    Returns:\n",
    "        A Pandas DataFrame of the dataset\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    return pd.read_csv(dataset_path)\n",
    "    ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f718f4a64c193b06c896b98e3cea6d23",
     "grade": true,
     "grade_id": "cell-85e9a7af94c49319",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# You can use this code cell to check if your pull_data function works correctly\n",
    "dataset_path = PARENT_DIR / \"bike_sharing_demand.csv\"\n",
    "df = pull_data(dataset_path)\n",
    "assert df.shape == (10886, 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d132a564e0073c43ccf2425f13c9eb4e",
     "grade": false,
     "grade_id": "cell-828e9bcc88609826",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10886 entries, 0 to 10885\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   season      10886 non-null  int64  \n",
      " 1   holiday     10886 non-null  int64  \n",
      " 2   workingday  10886 non-null  int64  \n",
      " 3   weather     10886 non-null  int64  \n",
      " 4   temp        10886 non-null  float64\n",
      " 5   atemp       10886 non-null  float64\n",
      " 6   humidity    10886 non-null  int64  \n",
      " 7   windspeed   10886 non-null  float64\n",
      " 8   count       10886 non-null  int64  \n",
      " 9   hour        10886 non-null  int64  \n",
      " 10  day         10886 non-null  int64  \n",
      " 11  month       10886 non-null  int64  \n",
      "dtypes: float64(3), int64(9)\n",
      "memory usage: 1020.7 KB\n"
     ]
    }
   ],
   "source": [
    "# Show a concise summary of the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cb401d8e2cd7821da245a6f6f22c42d",
     "grade": false,
     "grade_id": "cell-1a888284817aef8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>Expected output</summary>\n",
    "    <img src=\"./images/dataset-info.png\"/>\n",
    "</details>\n",
    "\n",
    "Below is the explanation of each column in the dataset:\n",
    "\n",
    "**Variables**:\n",
    "\n",
    "| Column name |  Explanation | type |\n",
    "|-------------|---------------|----|\n",
    "| season      | 1 = spring, 2 = summer, 3 = fall, 4 = winter | integer\n",
    "| holiday     | whether the day is considered a holiday | integer\n",
    "| workingday  | 1 if day is neither weekend nor holiday, otherwise 0. | integer\n",
    "| weather     | 1: Clear, Few clouds, Partly cloudy, Partly cloudy; 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist; 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds; 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog | integer\n",
    "| temp        | temperature in Celsius | float\n",
    "| atemp       | \"feels like\" temperature in Celsius | float\n",
    "| humidity    | relative humidity | integer\n",
    "| windspeed   | wind speed | float\n",
    "| hour        | the hours of the datetime| integer\n",
    "| day         | the day of the datetime| integer\n",
    "| month       | the month of the datetime| integer\n",
    "\n",
    "**Targets**: \n",
    "\n",
    "| Column name | Explanation                                     | Type\n",
    "|-------------|-------------------------------------------------| ----|\n",
    "| count       | number of total rentals                         | integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d1ae5c8ed9b20d5949a369323e642eb",
     "grade": false,
     "grade_id": "cell-dd0d9c7616b27d7e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1b) Split the data into train and test DataFrames\n",
    "Then implement the `splitData` function that splits the dataset into a training and a test dataset, using the last 168 rows of the dataset as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3ba8b026efe73b101b6149a0627ae53",
     "grade": false,
     "grade_id": "cell-8d442ea782a882c5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def splitData(input_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split a DataFrame into training and testing sets\n",
    "    Args:\n",
    "        input_df (DataFrame): The DataFrame to be splitted\n",
    "    Return:\n",
    "        A tuple of training and testing DataFrame\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "        # Split the last 168 rows as the test set\n",
    "    test_df = input_df.tail(168)\n",
    "    \n",
    "    # Use the remaining rows as the training set\n",
    "    train_df = input_df.iloc[:-168]\n",
    "    \n",
    "    return train_df, test_df\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab5b54dbeb01f4ee8a809027a91bfe42",
     "grade": false,
     "grade_id": "cell-2569727ff53704f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = pull_data(dataset_path)\n",
    "train, test = splitData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bca9f508f912e8ac796d507063c5b85",
     "grade": true,
     "grade_id": "cell-7ef629a52b6d83e4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check if train and test DataFrames are split correctly\n",
    "expected_train_shape = (10718, 12)\n",
    "expected_test_shape = (168, 12)\n",
    "\n",
    "assert (\n",
    "    train.shape == expected_train_shape\n",
    "), \"The dimension of the training dataset is not correct\"\n",
    "assert (\n",
    "    test.shape == expected_test_shape\n",
    "), \"The dimension of the testing dataset is not correct\"\n",
    "\n",
    "expected_columns = [\n",
    "    \"season\",\n",
    "    \"holiday\",\n",
    "    \"workingday\",\n",
    "    \"weather\",\n",
    "    \"temp\",\n",
    "    \"atemp\",\n",
    "    \"humidity\",\n",
    "    \"windspeed\",\n",
    "    \"count\",\n",
    "    \"hour\",\n",
    "    \"day\",\n",
    "    \"month\",\n",
    "]\n",
    "assert set(train.columns) == set(\n",
    "    expected_columns\n",
    "), \"The columns of the training dataset are not correct\"\n",
    "assert set(test.columns) == set(\n",
    "    expected_columns\n",
    "), \"The columns of the training dataset are not correct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ceb6675eac56e3a04c3eb8ffacf1704d",
     "grade": false,
     "grade_id": "cell-f8b0e77fc6d37be0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Split the training and testing DataFrames into features and targets\n",
    "target = \"count\"\n",
    "input_df = pull_data(PARENT_DIR / \"bike_sharing_demand.csv\")\n",
    "train, test = splitData(input_df)\n",
    "train_x = train.drop([target], axis=1)\n",
    "test_x = test.drop([target], axis=1)\n",
    "train_y = train[[target]]\n",
    "test_y = test[[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "911e67490f493579bb52ce67f7b45939",
     "grade": false,
     "grade_id": "cell-c4008413ccc15921",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 2: Offline model evaluation using Deepchecks (2 points)\n",
    "\n",
    "### 2a) Construct the Dataset objects used by Deepchecks\n",
    "First, let's construct the Deepchecks Dataset objects (named `train_dataset` and `test_dataset`) from the train and testing DataFrames.\n",
    "\n",
    "**Note**: Please use the `categorical_features` variable given below to specify the categorical features when you construct the datasets handled by Deepchecks. Please check [here](https://docs.deepchecks.com/stable/tabular/usage_guides/dataset_object.html) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1742c7620326f6eb1580404845840df3",
     "grade": false,
     "grade_id": "cell-8e2e13ec5a4dc257",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Categorical features, remember to specify the categorical features when you construct the datasets handled by Deepchecks\n",
    "# See https://docs.deepchecks.com/stable/tabular/usage_guides/dataset_object.html\n",
    "categorical_features = [\"season\", \"holiday\", \"workingday\", \"weather\", \"hour\", \"day\", \"month\"]\n",
    "\n",
    "# TODO: train_test = ...\n",
    "# test_dataset = ...\n",
    "### START CODE HERE\n",
    "train_dataset = Dataset(train_x, label=train_y, cat_features=categorical_features)\n",
    "test_dataset = Dataset(test_x, label=test_y, cat_features=categorical_features)\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd1b2d38b907dfb022b8af295cc367ef",
     "grade": true,
     "grade_id": "cell-ab60ceadab6629cd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Categorical features should be specified in train_dataset and test_dataset\n",
    "assert sorted(train_dataset.cat_features) == sorted(categorical_features), \"The categorical features of train_dataset are not specified correctly\"\n",
    "assert sorted(test_dataset.cat_features) == sorted(categorical_features),   \"The categorical features of test_dataset are not specified correctly\"\n",
    "\n",
    "# train_dataset and test_dataset should have the correct feature and label columns\n",
    "assert (train_dataset.features_columns.shape) == (10718, 11), \"The features columns of train_dataset are not correct\"\n",
    "assert (test_dataset.features_columns.shape) == (168, 11), \"The features columns of test_dataset are not correct\"\n",
    "assert (train_dataset.label_name) == \"count\", \"The label name of train_dataset is not correct\"\n",
    "assert (test_dataset.label_name) == \"count\", \"The label name of test_dataset is not correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a86a4e5038c4aab12672b0c95092474",
     "grade": false,
     "grade_id": "cell-435460895761e218",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2b) Deepchecks Suite with conditions\n",
    "Implement the `evaluate` function that uses Deepchecks Suite to perform the following two tests:\n",
    "1) Evaluate the model's MAE and RMSE on both training and testing dataset. This test should fail if the MAE or RMSE drops more than 20% on the testing dataset compared to the training dataset;\n",
    "2) Evaluate the model's inference time on both training and tests dataset. This test should fail if the average inference time exceeds 0.1 second. \n",
    "\n",
    "Finally, this function should return a Deepchecks [SuiteResult](https://docs.deepchecks.com/stable/api/generated/deepchecks.core.SuiteResult.html) containing the evaluation result.\n",
    "\n",
    "**Hints**:\n",
    "- [How to add conditions to a test?](https://docs.deepchecks.com/stable/general/usage/customizations/auto_examples/plot_configure_check_conditions.html)\n",
    "- [Train test performance](https://docs.deepchecks.com/stable/api/generated/deepchecks.tabular.checks.model_evaluation.TrainTestPerformance.html)\n",
    "- [Model inference time](https://docs.deepchecks.com/stable/tabular/auto_checks/model_evaluation/plot_model_inference_time.html)\n",
    "- [Condition for comparing model performance between training and testing dataset](https://docs.deepchecks.com/stable/api/generated/deepchecks.tabular.checks.model_evaluation.TrainTestPerformance.add_condition_train_test_relative_degradation_less_than.html#deepchecks.tabular.checks.model_evaluation.TrainTestPerformance.add_condition_train_test_relative_degradation_less_than)\n",
    "- [Condition for validating inference time](https://docs.deepchecks.com/stable/api/generated/deepchecks.tabular.checks.model_evaluation.ModelInferenceTime.add_condition_inference_time_less_than.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66f4cc06c7c771eb044c87e9983c2c50",
     "grade": false,
     "grade_id": "cell-062d639c98e46cfa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(train_dataset: Dataset, test_dataset: Dataset, model: LGBMRegressor) -> SuiteResult:\n",
    "    \"\"\"\n",
    "    Use Deepchecks to evaluate 1) model's MAE and RMSE on both training and testing dataset, 2) model's inference time.\n",
    "    Args:\n",
    "        train_dataset (Dataset): training Dataset\n",
    "        test_dataset (Dataset): testing Dataset\n",
    "        model (LGBMRegressor): The LightGBM regression model to be evaluated\n",
    "    Return:\n",
    "        a Deepchecks SuiteResult that contains the results of a Deepchecks suite run\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    # Initialize the train-test performance check\n",
    "    performance_check = TrainTestPerformance()\n",
    "    # Add condition to ensure that MAE and RMSE on test set do not degrade by more than 20% compared to training set\n",
    "    performance_check.add_condition_train_test_relative_degradation_less_than(0.2)\n",
    "    \n",
    "    # Initialize the model inference time check\n",
    "    inference_time_check = ModelInferenceTime()\n",
    "    # Add condition to ensure that average inference time is below 0.1 seconds\n",
    "    inference_time_check.add_condition_inference_time_less_than(0.1)\n",
    "\n",
    "    suite = Suite(\"Model Evaluation Suite\")\n",
    "    suite.add(performance_check)\n",
    "    suite.add(inference_time_check)\n",
    "    \n",
    "    # Run the suite on the provided model and datasets\n",
    "    suite_result = suite.run(train_dataset=train_dataset, test_dataset=test_dataset, model=model)\n",
    "    \n",
    "    return suite_result\n",
    "    ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73c49e2c5e7e7b3344487e3884c7cf2a",
     "grade": true,
     "grade_id": "cell-e6bd88c1926f291e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We provide a testing model trained on the same bike demand dataset to help you check if your evaluate function works correctly\n",
    "test_model = pickle.load(open(\"test-model.pkl\", \"rb\"))\n",
    "evaluation_result = evaluate(train_dataset, test_dataset, test_model)\n",
    "\n",
    "# These tests should pass\n",
    "failed_checks = evaluation_result.get_not_passed_checks()\n",
    "assert len(failed_checks) == 1, \"The number of failed checks in the evaluation result is not correct\"\n",
    "failed_condition_result = failed_checks[0].conditions_results[0]\n",
    "assert failed_condition_result.name == \"Train-Test scores relative degradation is less than 0.2\", \"The condition for comparing model performance between the training and test dataset is not correct\"\n",
    "\n",
    "passed_checks = evaluation_result.get_passed_checks()\n",
    "assert len(passed_checks) == 2, \"The number of passed checks in the evaluation result is not correct\"\n",
    "passed_condition_result = passed_checks[0].conditions_results[0]\n",
    "assert passed_condition_result.name == \"Average model inference time for one sample is less than 0.1\",  \"The condition for evaluating model inference time is not correct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bedc8a894c00f125e328c10ce7547081",
     "grade": false,
     "grade_id": "cell-ccf63aa5a742c532",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete the existing test-result.html\n",
      "The evaluation result is saved in an HTML file named test-result.html\n"
     ]
    }
   ],
   "source": [
    "# Export the evaluation results to an HTML file\n",
    "evaluation_result_file = \"test-result.html\"\n",
    "delete_file_if_existing(evaluation_result_file)\n",
    "\n",
    "evaluation_result_file = evaluation_result.save_as_html(evaluation_result_file)\n",
    "print(f\"The evaluation result is saved in an HTML file named {evaluation_result_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99b0e1d1ec4c9354b0c34081e07c1d26",
     "grade": false,
     "grade_id": "cell-be8d43d8c158c7cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After running the above code cell, you should see a file named \"test-result.html\" appear under the same directory as this notebook.\n",
    "\n",
    "<details>\n",
    "    <summary> Expected output when open the file in your browser </summary>\n",
    "    <br />\n",
    "    The test of MAE/RMSE should fail:\n",
    "    <br />\n",
    "    <img src=\"./images/deepchecks-train-test-performance.png\">\n",
    "    <br />\n",
    "    The test of inference time should pass:\n",
    "    <br />\n",
    "    <img src=\"./images/deepchecks-inference-time.png\">\n",
    "</details>\n",
    "\n",
    "### Screenshots to be submitted for Assignment 2\n",
    "Like the expected output above, please submit screenshots of the web page showing the passed and failed test(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1554c42f6f3d9b90314dfd13ab727180",
     "grade": false,
     "grade_id": "cell-0a41fadf1428a520",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 3: Tracking model training in MLflow (2 points)\n",
    "Similar to what you see in the MLflow tutorial, please complete the `log_to_mlflow` function that performs the following tasks:\n",
    "1. Use LightGBM to train a regression model to predict the bike sharing demand. The model should be trained using the training DataFrame you prepared previously and with the hyperparameters given as an argument. The hyperparameters are given as a dictionary, e.g. `hyperparams = {\"num_leaves\": 63, \"learning_rate\": 0.05, \"random_state\": 42}`. \n",
    "1. In an MLflow Run, use MLflow to track the model training:\n",
    "    1. Log the used hyperparameters to MLflow, using the keys in the `hyperparams` dictionary as the parameter names as shown below (check the \"Parameters\" column in the screenshot below).\n",
    "    2. Use the `evaluation` function you created above to evaluate the trained model. Then export the evaluation results to an HTML file and upload the file to MLflow. The HTML file of the Deepchecks model evaluation results uploaded to MLflow should be named **\"evaluation_result.html\"**. Please also make sure that the file you upload to MLflow doesn't locate inside any other directory under the MLflow Run.   \n",
    "    3. Register the trained model to MLflow. The registered model should be named **\"Week1LgbmBikeDemand\"** and linked to the MLflow Run. \n",
    "    4. Finally, return the Run ID of the MLflow Run. (You can refer to the MLflow tutorial on how to get the Run ID of an MLflow Run.)\n",
    "\n",
    "More illustration:\n",
    "\n",
    "<img src=\"./images/ass3-example.png\" width=1200/>\n",
    "\n",
    "Hints:\n",
    "* [Log multiple parameters to MLflow](https://mlflow.org/docs/2.9.2/python_api/mlflow.html#mlflow.log_params)\n",
    "* [Log a local file to MLflow](https://mlflow.org/docs/2.9.2/python_api/mlflow.html#mlflow.log_artifact)\n",
    "* [Log a model to MLflow](https://mlflow.org/docs/2.9.2/python_api/mlflow.lightgbm.html?highlight=log_model#mlflow.lightgbm.log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5f4cef080d4e6b6587d05cfe2c59066",
     "grade": false,
     "grade_id": "cell-fa5bfdfcb53813f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow/4', creation_time=1730535660424, experiment_id='4', last_update_time=1730535660424, lifecycle_stage='active', name='week1-lgbm-bike-demand', tags={}>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mlflow configuration\n",
    "MLFLOW_S3_ENDPOINT_URL = \"http://mlflow-minio.local\"\n",
    "MLFLOW_TRACKING_URI = \"http://mlflow-server.local\"\n",
    "AWS_ACCESS_KEY_ID = \"minioadmin\"\n",
    "AWS_SECRET_ACCESS_KEY = \"minioadmin\"\n",
    "mlflow_experiment_name = \"week1-lgbm-bike-demand\"\n",
    "\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = MLFLOW_S3_ENDPOINT_URL\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(mlflow_experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "558deebcbffc20ac7b1731a509b2ec6e",
     "grade": false,
     "grade_id": "cell-ad4fcf8a9b812e09",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mlflow_evaluation_result_filename = \"evaluation_result.html\"\n",
    "registered_model_name = \"Week1LgbmBikeDemand\"\n",
    "\n",
    "def log_to_mlflow(hyperparams: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Train a LightGBM model, log the used hyperparameters, upload the Deepchecks evaluation result HTML file and register the trained model to MLflow\n",
    "    Args:\n",
    "        hyperparams: The hyperparameters used to train the model\n",
    "    Returns:\n",
    "        The MLflow Run ID\n",
    "    \"\"\"\n",
    "    with mlflow.start_run() as run:\n",
    "        model = LGBMRegressor(**hyperparams)\n",
    "        model.fit(train_x, train_y, categorical_feature=categorical_features)\n",
    "\n",
    "        # TODO: 1) Log hyperparameters\n",
    "        # 2) Use the \"evaluate\" function to evaluate the model, export the evaluation results to an HTML file and upload the file\n",
    "        # 3) Register the model\n",
    "        # 4) Return the MLflow Run ID\n",
    "        ### START CODE HERE\n",
    "        # Log hyperparameters to MLflow\n",
    "        mlflow.log_params(hyperparams)\n",
    "        \n",
    "        # Evaluate the model and export the results to an HTML file\n",
    "        evaluation_result.save_as_html(mlflow_evaluation_result_filename)\n",
    "\n",
    "        # Log the HTML evaluation results file to MLflow\n",
    "        mlflow.log_artifact(mlflow_evaluation_result_filename)\n",
    "        \n",
    "        # Register the model with MLflow\n",
    "        mlflow.lightgbm.log_model(model, artifact_path=\"model\", registered_model_name=registered_model_name)\n",
    "        \n",
    "        # Get and return the Run ID of the MLflow run\n",
    "        run_id = run.info.run_id\n",
    "        return run_id\n",
    "        ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f4b18507296a52db0375ef3cb211109",
     "grade": false,
     "grade_id": "cell-37db502efdeccdb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete the existing evaluation_result.html\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 291\n",
      "[LightGBM] [Info] Number of data points in the train set: 10718, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 191.275518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/06 14:06:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'Week1LgbmBikeDemand' already exists. Creating a new version of this model...\n",
      "2024/11/06 14:06:57 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Week1LgbmBikeDemand, version 4\n",
      "Created version '4' of model 'Week1LgbmBikeDemand'.\n",
      "2024/11/06 14:06:57 INFO mlflow.tracking._tracking_service.client: 🏃 View run sedate-sponge-781 at: http://mlflow-server.local/#/experiments/4/runs/8a266fdee8424dd4b6da4881273162bf.\n",
      "2024/11/06 14:06:57 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://mlflow-server.local/#/experiments/4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Run ID: 8a266fdee8424dd4b6da4881273162bf\n"
     ]
    }
   ],
   "source": [
    "# model hyperparameters\n",
    "hyperparams = {\n",
    "    \"num_leaves\": 63,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "delete_file_if_existing(mlflow_evaluation_result_filename)\n",
    "mlflow_run_id = log_to_mlflow(hyperparams=hyperparams)\n",
    "print(f\"MLflow Run ID: {mlflow_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cc99ac91b42c50b1b2d0ab2d8c678fb",
     "grade": true,
     "grade_id": "cell-f4324a0d3b958193",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This test simply checks that your mlflow_run_id is not empty\n",
    "# Additional tests will be used during the grading so please make sure your implementation satisfies the requirements listed in the assignment instructions\n",
    "assert len(mlflow_run_id) != 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62c7b38b79b0b5cc6e28aa0f11c58a86",
     "grade": false,
     "grade_id": "cell-2e4762325cd2ed40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Screenshots to be submitted for Assignment 3\n",
    "To get the points from Assignment 3, please submit the following screenshots:\n",
    "1. The logs of your MLflow run. Please include the parameters of the model in your screenshot. \n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/mlflow-run.png\" width=1000>\n",
    "</details>\n",
    "\n",
    "2. The details of the MLflow run, including the uploaded Deepchecks evaluation result file;\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/mlflow-run-detail1.png\" width=1000>\n",
    "    <img src=\"./images/mlflow-run-detail2.png\" width=1000>\n",
    "    <img src=\"./images/mlflow-run-detail3.png\" width=1000>\n",
    "</details>\n",
    "\n",
    "3. The registered model.\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/mlflow-model.png\" width=1000>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "937bfbb66691ae9c9844a0ef6524219e",
     "grade": false,
     "grade_id": "cell-be8b87e6241bded7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 4: Evaluate the trained model against another model (2 points)\n",
    "Suppose your colleague had trained an ElasticNet model for the same use case of bike sharing demand prediction. In this assignment, please complete the `compare_models` function that performs the following tasks:\n",
    "1. Using Deepchecks [Multi model performance report](https://docs.deepchecks.com/stable/tabular/auto_checks/model_evaluation/plot_multi_model_performance_report.html) to compare MAE and RMSE of your LightGBM model to the ElasticNet model and saving the result as an HTML file. **Use negative MAE and negative RMSE as the scorers**.\n",
    "1. Uploading the result file to MLflow, the file should be named **\"model_comparison.html\"** and under the MLflow Run where you trained your LightGBM model in Assignment 3. Similar to `evaluation_result.html`, `model_comparison.html` shouldn't be inside any other folder.\n",
    "E.g.,\n",
    "\n",
    "<img src=\"./images/ass4-example.png\" width=300 />\n",
    "\n",
    "`model_comparison.html` should look like this:\n",
    "\n",
    "<img src=\"./images/deepchecks-compare-models.png\" >\n",
    "\n",
    "Finally, the function should return a Deepckecks [CheckResult](https://docs.deepchecks.com/stable/api/generated/deepchecks.core.CheckResult.html) containing the model comparison results. \n",
    "\n",
    "Note that the idea here is to attach a file to an existing MLflow Run, not to create a new MLflow Run and then upload the file under the new MLflow Run. \n",
    "\n",
    "You may find the following doc helpful: \n",
    "- [mlflow.start_run](https://mlflow.org/docs/2.9.2/python_api/mlflow.html#mlflow.start_run) (Pay attention to the use of the `run_id` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76d0d1381fa2029c260e31f1679412df",
     "grade": false,
     "grade_id": "cell-40842f07c90700f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 291\n",
      "[LightGBM] [Info] Number of data points in the train set: 10718, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 191.275518\n"
     ]
    }
   ],
   "source": [
    "# This is the model you just trained. In practice, the model can be downloaded from MLflow. Here, for simplicity, we just retrain the model\n",
    "model = LGBMRegressor(**hyperparams)\n",
    "model.fit(train_x, train_y, categorical_feature=categorical_features)\n",
    "\n",
    "# Load the old ElasticNet model\n",
    "old_model = pickle.load(open(\"old-model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "820718eb8d1c97925ede96e12a4458d7",
     "grade": false,
     "grade_id": "cell-dbc07f15dbab9c86",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mlflow_model_comparison_result_filename = \"model_comparison.html\"\n",
    "\n",
    "def compare_models(mlflow_run_id: str) -> CheckResult:\n",
    "    \"\"\"\n",
    "    Use Deepchecks to compare the performance of the LightGBM model and the old ElasticNet model\n",
    "    Args:\n",
    "        mlflow_run_id: The model comparison result file should be uploaded under the MLflow Run whose Run ID is mlflow_run_id\n",
    "    Return:\n",
    "        Deepchecks CheckResult that contains the model comparison results\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: 1) Use Deepchecks to compare your LightGBM model to the ElasticNet model and save the result to an HTML file\n",
    "    # 2) Upload the result file to MLflow. The file should be under the MLflow Run where you trained your LightGBM model\n",
    "    # 3) Return the comparison CheckResult\n",
    "    ### START CODE HERE\n",
    "    ## 创建一个 Deepchecks DataSet\n",
    "    train_ds = Dataset(train_x, train_y, cat_features=categorical_features)\n",
    "    test_ds = Dataset(test_x, test_y, cat_features=categorical_features)\n",
    "    \n",
    "    # 使用 Deepchecks 进行模型性能比较\n",
    "    report = MultiModelPerformanceReport()\n",
    "    result = report.run(train_ds, test_ds, [model, old_model])\n",
    "    \n",
    "    # 打印结果的 DataFrame 内容\n",
    "    res_df = result.value\n",
    "    print(\"DataFrame content:\", res_df)\n",
    "    print(\"DataFrame columns:\", res_df.columns)\n",
    "    \n",
    "    # 保存比较结果为 HTML 文件\n",
    "    result.save_as_html(mlflow_model_comparison_result_filename)\n",
    "    \n",
    "    # 在指定的 MLflow Run 中上传结果文件\n",
    "    with mlflow.start_run(run_id=mlflow_run_id):\n",
    "        mlflow.log_artifact(mlflow_model_comparison_result_filename)\n",
    "    \n",
    "    # 返回 Deepchecks 的检查结果\n",
    "    return result\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0092b7db7e1bdd82dd60c043d1d1bda",
     "grade": false,
     "grade_id": "cell-446fb5061b5b7992",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete the existing model_comparison.html\n"
     ]
    }
   ],
   "source": [
    "delete_file_if_existing(mlflow_model_comparison_result_filename)\n",
    "\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = MLFLOW_S3_ENDPOINT_URL\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c6e9a93937a798ebd6d50df6b8325bd",
     "grade": true,
     "grade_id": "cell-44d3debf75894f17",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame content:            Model       Value    Metric  Number of samples\n",
      "0  LGBMRegressor  -73.494141  Neg RMSE                168\n",
      "1  LGBMRegressor  -54.707325   Neg MAE                168\n",
      "2  LGBMRegressor    0.812085        R2                168\n",
      "3     ElasticNet -154.583432  Neg RMSE                168\n",
      "4     ElasticNet -108.537966   Neg MAE                168\n",
      "5     ElasticNet    0.168655        R2                168\n",
      "DataFrame columns: Index(['Model', 'Value', 'Metric', 'Number of samples'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/06 14:31:49 INFO mlflow.tracking._tracking_service.client: 🏃 View run sedate-sponge-781 at: http://mlflow-server.local/#/experiments/4/runs/8a266fdee8424dd4b6da4881273162bf.\n",
      "2024/11/06 14:31:49 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://mlflow-server.local/#/experiments/4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame content:            Model       Value    Metric  Number of samples\n",
      "0  LGBMRegressor  -73.494141  Neg RMSE                168\n",
      "1  LGBMRegressor  -54.707325   Neg MAE                168\n",
      "2  LGBMRegressor    0.812085        R2                168\n",
      "3     ElasticNet -154.583432  Neg RMSE                168\n",
      "4     ElasticNet -108.537966   Neg MAE                168\n",
      "5     ElasticNet    0.168655        R2                168\n",
      "DataFrame columns: Index(['Model', 'Value', 'Metric', 'Number of samples'], dtype='object')\n",
      "Delete the existing model_comparison.html\n"
     ]
    }
   ],
   "source": [
    "res = compare_models(mlflow_run_id=mlflow_run_id)\n",
    "\n",
    "# Check that the returned CheckResult is correct\n",
    "res_df = res.value\n",
    "print(\"DataFrame content:\", res_df)\n",
    "print(\"DataFrame columns:\", res_df.columns)\n",
    "lgbm_neg_mae = res_df.loc[\n",
    "    (res_df[\"Model\"] == \"LGBMRegressor\") & (res_df[\"Metric\"] == \"Neg MAE\")\n",
    "][\"Value\"].values[0]\n",
    "elasticnet_neg_mae = res_df.loc[\n",
    "    (res_df[\"Model\"] == \"ElasticNet\") & (res_df[\"Metric\"] == \"Neg MAE\")\n",
    "][\"Value\"].values[0]\n",
    "assert (\n",
    "    lgbm_neg_mae > elasticnet_neg_mae\n",
    "), \"The Deepchecks model comparison report is not correct. The negative MAE of the LightGBM model should be larger than the negative MAE of the ElasticNet model\"\n",
    "\n",
    "lgbm_neg_rmse = res_df.loc[\n",
    "    (res_df[\"Model\"] == \"LGBMRegressor\") & (res_df[\"Metric\"] == \"Neg RMSE\")\n",
    "][\"Value\"].values[0]\n",
    "elasticnet_neg_rmse = res_df.loc[\n",
    "    (res_df[\"Model\"] == \"ElasticNet\") & (res_df[\"Metric\"] == \"Neg RMSE\")\n",
    "][\"Value\"].values[0]\n",
    "assert (\n",
    "    lgbm_neg_rmse > elasticnet_neg_rmse\n",
    "), \"The Deepchecks model comparison report is not correct. The negative RMSE of the LightGBM model should be larger than the negative RMSE of the ElasticNet model\"\n",
    "\n",
    "delete_file_if_existing(mlflow_model_comparison_result_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c26b57c8994eb170725651c92359fa2a",
     "grade": false,
     "grade_id": "cell-117472936de658f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Screenshots to be submitted for Assignment 4\n",
    "The details of the MLflow run including uploaded Deepchecks model comparison result file.\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/deepchecks-compare-models.png\" />\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba1d48fb421edba655ea9bd6a67916e5",
     "grade": false,
     "grade_id": "cell-ce3e79750bf40971",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## What to submit\n",
    "- This Jupyter notebook\n",
    "- The PDF file containing your screenshots for Assignments 2, 3, and 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
