{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Several YAML files will be used to run some examples in this tutorial. Similar to what you did before, you will need to change the \"storareUri\" in these YAML files to the S3 URI of one of your own red wine models saved in MLflow if you want to run the examples yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizontal scaling\n",
    "The capacity of a single server to handle computational tasks is limited. As user traffic surges, it becomes necessary to distribute this traffic among multiple replicas that run your model on different servers. \n",
    "\n",
    "**Horizontal scaling** means adding more servers to ensure that your inference service can respond to increasing user requests.\n",
    "\n",
    "In our K8s setup, horizontal scaling in practice means increasing the pods running for an inference service. In [manifests/redwine-model-scale.yaml](./manifests/redwine-model-scale.yaml), by adding the `minReplicas` field, we specify the minimum number of pods running for the \"redwine-week4\" inference service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/redwine-week4 created\n"
     ]
    }
   ],
   "source": [
    "# Create the \"redwine-week4\" inference service served by 3 pods.\n",
    "!kubectl apply -f manifests/redwine-model-scale.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/redwine-week4 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION             AGE\n",
      "redwine-week4   http://redwine-week4.kserve-inference.example.com   True           100                              redwine-week4-predictor-00001   36s\n"
     ]
    }
   ],
   "source": [
    "# Check the \"redwine-week4\" inference service to make sure it's ready\n",
    "!kubectl get isvc redwine-week4 -n kserve-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                     AGE\n",
    "redwine-week4   http://redwine-week4.kserve-inference.example.com   True           100                              redwine-week4-predictor-default-00001   28s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                      READY   STATUS    RESTARTS   AGE   IP             NODE              NOMINATED NODE   READINESS GATES\n",
      "redwine-week4-predictor-00001-deployment-d567b64b-cztdt   2/2     Running   0          37s   10.244.2.158   kind-ep-worker    <none>           <none>\n",
      "redwine-week4-predictor-00001-deployment-d567b64b-d7c5q   2/2     Running   0          36s   10.244.2.159   kind-ep-worker    <none>           <none>\n",
      "redwine-week4-predictor-00001-deployment-d567b64b-l4f2w   2/2     Running   0          36s   10.244.1.207   kind-ep-worker2   <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "# Check the pods running for the \"redwine-week4\" inference service\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=redwine-week4 -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE     IP             NODE              NOMINATED NODE   READINESS GATES\n",
    "redwine-week4-predictor-default-00002-deployment-5744cfb98j88x2   2/2     Running   0          4m27s   10.244.2.106   kind-ep-worker2   <none>           <none>\n",
    "redwine-week4-predictor-default-00002-deployment-5744cfb98qrzqq   2/2     Running   0          4m27s   10.244.2.107   kind-ep-worker2   <none>           <none>\n",
    "redwine-week4-predictor-default-00002-deployment-5744cfb98z5lvb   2/2     Running   0          4m27s   10.244.1.120   kind-ep-worker    <none>           <none>\n",
    "```\n",
    "The \"NODE\" field shows that these pods are running on different cluster nodes. The IPs in the \"IP\" column can vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"redwine-week4\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "!kubectl -n kserve-inference delete isvc redwine-week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"redwine-week4\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal autoscaling\n",
    "User traffic can fluctuate up and down in many use cases. As a result, it is inefficient to manually add more servers when traffic spikes come and remove them when traffic decreases. This is where horizontal autoscaling is needed. By horizontal autoscaling, new servers can be automatically launched when user traffic increases. Similarly, excess servers can be terminated when the traffic decreases. \n",
    "\n",
    "Autoscaling can bring the following benefits:\n",
    "- It allows your inference service to automatically respond changes in demand, minimizing human intervention.\n",
    "- Servers can allocated based on user demand, minimizing the risks of overprovisioning (i.e., launching too many servers than actually needed), thereby saving costs during periods of low demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horizontal autoscaling in KServe\n",
    "KServe offers horizontal autoscaling by default. Let's look at an example in [manifests/redwine-model-autoscale.yaml](./manifests/redwine-model-autoscale.yaml). The content looks familiar, except the following fields:\n",
    "```yaml\n",
    "spec:\n",
    "  predictor:\n",
    "    minReplicas: 1\n",
    "    maxReplicas: 8\n",
    "    scaleTarget: 1\n",
    "    scaleMetric: concurrency\n",
    "```\n",
    "- `minReplicas`: Minimum number of pods running for an inference service.\n",
    "- `maxReplicas`: Maximum number of pods running for an inference service after autoscaling.\n",
    "- `scaleMetric`: The scaling metric. It's used to decide when to scale pods. In this example, the metric is concurrency, which refers to the number of in-flight requests per pod at any given time.\n",
    "- `scaleTarget`: This is an integer that specifies the target value of the \"scaleMetric\" that autoscaling should try to satisfy. Note that this target may not always be achieved as the maximum number of pods and/or the computational resources in the cluster is limited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/redwine-week4 created\n"
     ]
    }
   ],
   "source": [
    "# Create the \"redwine-week4\" inference service with horizontal autoscaling enabled\n",
    "!kubectl apply -f manifests/redwine-model-autoscale.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/redwine-week4 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            URL   READY     PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION   AGE\n",
      "redwine-week4         Unknown                                                                 5s\n",
      "redwine-week4         Unknown                                                                 33s\n",
      "redwine-week4         Unknown          100                              redwine-week4-predictor-00001   33s\n",
      "redwine-week4         Unknown          100                              redwine-week4-predictor-00001   33s\n",
      "redwine-week4   http://redwine-week4.kserve-inference.example.com   True             100                              redwine-week4-predictor-00001   33s\n",
      "redwine-week4   http://redwine-week4.kserve-inference.example.com   True             100                              redwine-week4-predictor-00001   33s\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Make sure the \"redwine-week4\" inference service is ready\n",
    "!kubectl get isvc redwine-week4 -n kserve-inference -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "NAME            URL                                                 READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                     AGE\n",
    "redwine-week4   http://redwine-week4.kserve-inference.example.com   True           100                              redwine-week4-predictor-default-00001   36s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                        READY   STATUS    RESTARTS   AGE\n",
      "redwine-week4-predictor-00001-deployment-5f56c9f4b7-87qsd   2/2     Running   0          35s\n"
     ]
    }
   ],
   "source": [
    "# Check the pods running for the \"redwine-week4\" inference service\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=redwine-week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85b9sdcf   2/2     Running   0          68s\n",
    "```\n",
    "Only one pod is running for the \"redwine-week4\" inference service now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's send some requests to the service using a tool named \"hey\", which is a small program that send traffic in a controllable way. You can install it from [its repository](https://github.com/rakyll/hey).\n",
    "\n",
    "We will use the following command to simulate user requests:\n",
    "```bash\n",
    "hey -z 10s -c 5 -m POST -host ${host} -D ${input_path} ${url}\n",
    "```\n",
    "Running this command will simulate 5 concurrent POST requests for 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  Total:\t10.0184 secs\n",
      "  Slowest:\t0.1859 secs\n",
      "  Fastest:\t0.0166 secs\n",
      "  Average:\t0.0252 secs\n",
      "  Requests/sec:\t198.5338\n",
      "  \n",
      "  Total data:\t421668 bytes\n",
      "  Size/request:\t212 bytes\n",
      "\n",
      "Response time histogram:\n",
      "  0.017 [1]\t|\n",
      "  0.034 [1786]\t|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
      "  0.050 [177]\t|■■■■\n",
      "  0.067 [18]\t|\n",
      "  0.084 [2]\t|\n",
      "  0.101 [0]\t|\n",
      "  0.118 [0]\t|\n",
      "  0.135 [0]\t|\n",
      "  0.152 [0]\t|\n",
      "  0.169 [0]\t|\n",
      "  0.186 [5]\t|\n",
      "\n",
      "\n",
      "Latency distribution:\n",
      "  10% in 0.0188 secs\n",
      "  25% in 0.0203 secs\n",
      "  50% in 0.0227 secs\n",
      "  75% in 0.0269 secs\n",
      "  90% in 0.0338 secs\n",
      "  95% in 0.0391 secs\n",
      "  99% in 0.0552 secs\n",
      "\n",
      "Details (average, fastest, slowest):\n",
      "  DNS+dialup:\t0.0000 secs, 0.0166 secs, 0.1859 secs\n",
      "  DNS-lookup:\t0.0000 secs, 0.0000 secs, 0.0025 secs\n",
      "  req write:\t0.0001 secs, 0.0000 secs, 0.0003 secs\n",
      "  resp wait:\t0.0249 secs, 0.0165 secs, 0.1638 secs\n",
      "  resp read:\t0.0001 secs, 0.0000 secs, 0.0005 secs\n",
      "\n",
      "Status code distribution:\n",
      "  [500]\t1989 responses\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "model_name=redwine-week4\n",
    "input_path=redwine-input.json\n",
    "host=${model_name}.kserve-inference.example.com\n",
    "url=http://kserve-gateway.local:30200/v1/models/${model_name}:predict\n",
    "\n",
    "# Send 10 seconds of post requests maintaining 5 in-flight requests. The sent data are saved in redwine-input.json\n",
    "hey -z 10s -c 5 -m POST -host ${host} -D ${input_path} ${url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                        READY   STATUS        RESTARTS   AGE\n",
      "redwine-week4-predictor-00001-deployment-5f56c9f4b7-87qsd   2/2     Terminating   0          117s\n",
      "redwine-week4-predictor-00001-deployment-5f56c9f4b7-pk95v   2/2     Running       0          73s\n",
      "redwine-week4-predictor-00001-deployment-5f56c9f4b7-vrp6x   2/2     Terminating   0          67s\n"
     ]
    }
   ],
   "source": [
    "# Check the pods running for the \"redwine-week4\" inference service again\n",
    "# You need to run this command immediately after the hey command is completed\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=redwine-week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85b5gzxw   2/2     Running   0          24s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85b9sdcf   2/2     Running   0          70s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85bhtvfx   2/2     Running   0          22s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85brm5xb   2/2     Running   0          22s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85bxpnp5   2/2     Running   0          20s\n",
    "```\n",
    "Recall that we set `scaleTarget` to 1 in manifests/redwine-model-autoscale.yaml, which means KServe should try to scale up the pods in such a way that each pod will handle one request simultaneously. Since 5 concurrent requests were sent, five pods were needed in total. As a result, KServe launched four more pods to handle the requests. \n",
    "\n",
    "Notice that the number of the additional pods KServe creates can vary depending on how the traffic arrived at the inference service. It's also OK if you see less or more than four additional pods created.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wait for a few minutes and check the pods again (by running the previous code cell), you will see four of pods are being terminated. In other words, the inference service is scaled down as there is no traffic anymore.\n",
    "```text\n",
    "NAME                                                              READY   STATUS        RESTARTS   AGE\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85b5gzxw   2/2     Terminating   0          76s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85b9sdcf   2/2     Running       0          2m2s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85bhtvfx   2/2     Terminating   0          74s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85brm5xb   2/2     Terminating   0          74s\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85bxpnp5   2/2     Terminating   0          72s\n",
    "```\n",
    "Finally, only one pod remains:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "redwine-week4-predictor-default-00001-deployment-5ddb7c85b9sdcf   2/2     Running       0     4m27s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"redwine-week4\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "!kubectl -n kserve-inference delete isvc redwine-week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"redwine-week4\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Credits: The examples are based on [this KServe doc](https://kserve.github.io/website/0.10/modelserving/autoscaling/autoscaling/).*\n",
    "\n",
    "*If you are interested at seeing how horizontal autoscaling can maintain the performance of an inference service, you can take a look at [this experiment](./autoscaling-exp.ipynb). (You don't need to run the experiment yourself. The experiment is used to illustrate how horizontal autoscaling can maintain the performance of an inference service in user traffic spikes.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step\n",
    "You've learned how to horizontally scale an inference service using KServe. Now you can go to [the next tutorial](./4_inference_graph.ipynb) and learn about inference graph. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
