{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week4 Assignments (part2)\n",
    "This is the second part of this week's assignments. **Please run this notebook using the `mlops_eng2` environment** (it should be created when you follow the tutorials).\n",
    "\n",
    "### Guidelines for doing Assignments 2-5\n",
    "- In 2a), you'll need to write some Python code, so please put your code between the `### START CODE HERE` and `### END CODE HERE` comments. Please **do not change any code other than those between the `### START CODE HERE` and `### END CODE HERE` comments**. \n",
    "- In other assignments, you'll need to complete some configurations in YAML files. In each YAML file, please write your configurations between the `### START CONF HERE` and `### END CONF HERE` comments. Again, please **do not change any text other than those between the `### START CONF HERE` and `### END CONF HERE` comments**. \n",
    "- You will use a command `kubectl -n kserve-inference get isvc <name-of-inference-service>` (or `kubectl -n kserve-inference get ig <name-of-inference-graph>`) a few times when running this notebook. This command checks whether your inference service (or inference graph) deployed to KServe is ready. It takes some time (up to a few minutes) for a inference service/graph to become ready, so you may need to run the same command a few times to follow the readiness of your inference service/graph. You can also use the \"-w\" option to continuously watch the status of the inference service/graph (`kubectl get isvc <name-of-inference-service> -n kserve-inference -w`) and then terminate the code cell when the inference service/graph is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Deploy a model to KServe (3 points)\n",
    "In this assignment, you need to deploy your LightGBM model for predicting bike sharing demand as an inference service to KServe. You can use the model you just trained before starting the first assignment. \n",
    "\n",
    "Similar to the tutorial, the deployed inference service should run in the \"kserve-inference\" namespace and the service account name containing the credentials for accessing the MinIO storage service is also \"kserve-sa\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kserve_utils import send_request\n",
    "from utils.common_utils import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you're using the correct version of lightgbm\n",
    "import lightgbm\n",
    "assert lightgbm.__version__ == \"3.3.5\", \"Your lightgbm version is not 3.3.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) Use Python SDK to deploy your LightGBM model\n",
    "Complete the `deploy_model` function that uses the KServe SDK to deploy your LightGBM model. If there is no model deployed, your function should create a new inference service; if there is an inference service existing, your function should be able to update it. \n",
    "\n",
    "**Hint**: Using the LightGBM server provided by KServe doesn't work because the model saved by MLflow is in the pickled format, which is different from the format supported by KServe's LightGBM server. You can check [here](https://github.com/kserve/kserve/issues/2483) on how to use KServe SDK to deploy a model uploaded to MLflow.\n",
    "\n",
    "After complete and run the next code cell, you should see the code in the code cell exported to a Python script named `part2_answer.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b2869e092e9e48cfa4ff7a81515aee2",
     "grade": false,
     "grade_id": "cell-0507bc7b34f2190c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting part2_answer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile part2_answer.py\n",
    "\n",
    "from kubernetes import client\n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import V1beta1InferenceService\n",
    "from kserve import V1beta1InferenceServiceSpec\n",
    "from kserve import V1beta1PredictorSpec\n",
    "from kserve import V1beta1ModelSpec\n",
    "from kserve import V1beta1ModelFormat\n",
    "\n",
    "def deploy_model(model_name: str, model_uri: str):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_name: the name of the deployed inference service\n",
    "        model_uri: the S3 URI of the model saved in MLflow\n",
    "    \"\"\"\n",
    "    \n",
    "    namespace = \"kserve-inference\"\n",
    "    service_account_name=\"kserve-sa\"\n",
    "    kserve_version=\"v1beta1\"\n",
    "    api_version = constants.KSERVE_GROUP + \"/\" + kserve_version\n",
    "    \n",
    "    print(f\"MODEL URI: {model_uri}\")\n",
    "    \n",
    "    modelspec = V1beta1ModelSpec(\n",
    "        storage_uri=model_uri,\n",
    "        model_format=V1beta1ModelFormat(name=\"mlflow\"),\n",
    "        protocol_version=\"v2\"\n",
    "    )\n",
    "    \n",
    "    isvc = V1beta1InferenceService(\n",
    "\n",
    "        ### START CODE HERE\n",
    "        api_version=api_version,\n",
    "        kind=\"InferenceService\",\n",
    "        metadata=client.V1ObjectMeta(name=model_name, namespace=namespace),\n",
    "        ### END CODE HERE\n",
    "\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                ### START CODE HERE\n",
    "                model=modelspec,\n",
    "                service_account_name=service_account_name\n",
    "                ### END CODE HERE\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    kserve = KServeClient()\n",
    "\n",
    "    ### START CODE HERE\n",
    "    # Check if the model is already deployed, if yes, update it; if no, create a new service\n",
    "    try:\n",
    "        existing_service = kserve.get(model_name, namespace=namespace)\n",
    "        print(f\"Service {model_name} already exists, updating...\")\n",
    "        kserve.patch(isvc)\n",
    "    except Exception as e:\n",
    "        print(f\"Service {model_name} not found, creating a new one...\")\n",
    "        kserve.create(isvc, namespace=namespace)\n",
    "    ### END CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found, skip training and use the existing model s3://mlflow/7/c6593f7cd39f4444acd8581588de91af/artifacts/lgbm-bike\n",
      "MODEL URI: s3://mlflow/7/c6593f7cd39f4444acd8581588de91af/artifacts/lgbm-bike\n",
      "Service bike-lgbm-2a not found, creating a new one...\n"
     ]
    }
   ],
   "source": [
    "from part2_answer import deploy_model\n",
    "\n",
    "model_name = \"bike-lgbm-2a\"\n",
    "\n",
    "params = {\"num_leaves\": 63, \"learning_rate\": 0.05, \"random_state\": 42}\n",
    "model_uri = train(model_type=\"lgbm\", model_params=params, freshness_tag=\"old\")\n",
    "\n",
    "# Test the deploy_model function\n",
    "deploy_model(model_name, model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           URL                                                READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION            AGE\n",
      "bike-lgbm-2a   http://bike-lgbm-2a.kserve-inference.example.com   True           100                              bike-lgbm-2a-predictor-00001   16m\n"
     ]
    }
   ],
   "source": [
    "# Check if the \"bike-lgbm-2a\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME           URL                                                READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                    AGE\n",
    "bike-lgbm-2a   http://bike-lgbm-2a.kserve-inference.example.com   True           100                              bike-lgbm-2a-predictor-default-00001   72s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                       READY   STATUS    RESTARTS   AGE\n",
      "bike-lgbm-2a-predictor-00001-deployment-774ffbcd5c-dkjf4   2/2     Running   0          16m\n"
     ]
    }
   ],
   "source": [
    "# Make sure there is one pod running for the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-2a-predictor-default-00001-deployment-6499598b7-wc28j   2/2     Running   0          65s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'bike-lgbm-2a', 'id': '2f6fbd16-244e-458f-a696-d2caf6098bf7', 'parameters': {}, 'outputs': [{'name': 'output-1', 'shape': [2, 1], 'datatype': 'FP64', 'data': [51.00457318737209, 35.13687405851507]}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'bike-lgbm-2a',\n",
       " 'id': '2f6fbd16-244e-458f-a696-d2caf6098bf7',\n",
       " 'parameters': {},\n",
       " 'outputs': [{'name': 'output-1',\n",
       "   'shape': [2, 1],\n",
       "   'datatype': 'FP64',\n",
       "   'data': [51.00457318737209, 35.13687405851507]}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send a request to the inference service\n",
    "send_request(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "{'model_name': 'bike-lgbm-2a',\n",
    " 'id': 'eddb6d4b-e517-421a-8420-d02db301428b',\n",
    " 'parameters': {},\n",
    " 'outputs': [{'name': 'output-1',\n",
    "   'shape': [2, 1],\n",
    "   'datatype': 'FP64',\n",
    "   'data': [51.00457318737209, 35.13687405851507]}]}\n",
    "```\n",
    "**Note**: The id varies. The important point is that the response has the correct fields as shown in the above expected output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P.S.* KServe also uses MLServer to serve the models uploaded to the MLflow service, which means your inference service also uses the V2 inference protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's train another model with different hyperparameters and see if your `deploy_model` function can update the existing inference service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model found, start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in environment variables.\n",
      "Registered model 'Week4LgbmBikeDemand' already exists. Creating a new version of this model...\n",
      "2024/11/28 15:02:59 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Week4LgbmBikeDemand, version 2\n",
      "Created version '2' of model 'Week4LgbmBikeDemand'.\n",
      "/Users/liupei/Desktop/engineering_of_ml_systems/exe_week4/week4_tutorial_assignments/week4_assignments/utils/common_utils.py:135: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.9.2/model-registry.html#migrating-from-stages\n",
      "  model_info = MLFLOW_CLIENT.get_latest_versions(registered_model_name)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained model is located at s3://mlflow/7/46df22f2a80042468a81d304db06896b/artifacts/lgbm-bike\n",
      "MODEL URI: s3://mlflow/7/46df22f2a80042468a81d304db06896b/artifacts/lgbm-bike\n",
      "Service bike-lgbm-2a already exists, updating...\n",
      "Service bike-lgbm-2a not found, creating a new one...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception when calling CustomObjectsApi->create_namespaced_custom_object:                 (409)\nReason: Conflict\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '2eb8aaf5-317c-40a4-aee3-c1f08dc4c0be', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'X-Kubernetes-Pf-Flowschema-Uid': '7de4939b-bceb-4511-b9e3-2bd606c1798f', 'X-Kubernetes-Pf-Prioritylevel-Uid': 'b71e55c5-88fe-4fee-8b3a-546ff7e59472', 'Date': 'Thu, 28 Nov 2024 13:02:59 GMT', 'Content-Length': '274'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"inferenceservices.serving.kserve.io \\\"bike-lgbm-2a\\\" already exists\",\"reason\":\"AlreadyExists\",\"details\":{\"name\":\"bike-lgbm-2a\",\"group\":\"serving.kserve.io\",\"kind\":\"inferenceservices\"},\"code\":409}\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/engineering_of_ml_systems/exe_week4/week4_tutorial_assignments/week4_assignments/part2_answer.py:55\u001b[0m, in \u001b[0;36mdeploy_model\u001b[0;34m(model_name, model_uri)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists, updating...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mkserve\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43misvc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: KServeClient.patch() missing 1 required positional argument: 'inferenceservice'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/kserve/api/kserve_client.py:118\u001b[0m, in \u001b[0;36mKServeClient.create\u001b[0;34m(self, inferenceservice, namespace, watch, timeout_seconds)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_namespaced_custom_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKSERVE_GROUP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKSERVE_PLURAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43minferenceservice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m client\u001b[38;5;241m.\u001b[39mrest\u001b[38;5;241m.\u001b[39mApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/kubernetes/client/api/custom_objects_api.py:225\u001b[0m, in \u001b[0;36mCustomObjectsApi.create_namespaced_custom_object\u001b[0;34m(self, group, version, namespace, plural, body, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_return_http_data_only\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_namespaced_custom_object_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplural\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/kubernetes/client/api/custom_objects_api.py:344\u001b[0m, in \u001b[0;36mCustomObjectsApi.create_namespaced_custom_object_with_http_info\u001b[0;34m(self, group, version, namespace, plural, body, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m auth_settings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearerToken\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/apis/\u001b[39;49m\u001b[38;5;132;43;01m{group}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{version}\u001b[39;49;00m\u001b[38;5;124;43m/namespaces/\u001b[39;49m\u001b[38;5;132;43;01m{namespace}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{plural}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mform_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_formats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/kubernetes/client/api_client.py:348\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api, (resource_path,\n\u001b[1;32m    356\u001b[0m                                                method, path_params,\n\u001b[1;32m    357\u001b[0m                                                query_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                                _request_timeout,\n\u001b[1;32m    366\u001b[0m                                                _host))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/kubernetes/client/api_client.py:180\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response_data\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/kubernetes/client/api_client.py:391\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/kubernetes/client/rest.py:276\u001b[0m, in \u001b[0;36mRESTClientObject.POST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPOST\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, query_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, post_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m          body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/kubernetes/client/rest.py:235\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m299\u001b[39m:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mApiException\u001b[0m: (409)\nReason: Conflict\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '2eb8aaf5-317c-40a4-aee3-c1f08dc4c0be', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'X-Kubernetes-Pf-Flowschema-Uid': '7de4939b-bceb-4511-b9e3-2bd606c1798f', 'X-Kubernetes-Pf-Prioritylevel-Uid': 'b71e55c5-88fe-4fee-8b3a-546ff7e59472', 'Date': 'Thu, 28 Nov 2024 13:02:59 GMT', 'Content-Length': '274'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"inferenceservices.serving.kserve.io \\\"bike-lgbm-2a\\\" already exists\",\"reason\":\"AlreadyExists\",\"details\":{\"name\":\"bike-lgbm-2a\",\"group\":\"serving.kserve.io\",\"kind\":\"inferenceservices\"},\"code\":409}\n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m new_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_leaves\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m31\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m}\n\u001b[1;32m      2\u001b[0m new_model_s3_uri \u001b[38;5;241m=\u001b[39m train(model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlgbm\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_params\u001b[38;5;241m=\u001b[39mnew_params, freshness_tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdeploy_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_model_s3_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/engineering_of_ml_systems/exe_week4/week4_tutorial_assignments/week4_assignments/part2_answer.py:58\u001b[0m, in \u001b[0;36mdeploy_model\u001b[0;34m(model_name, model_uri)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found, creating a new one...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mkserve\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43misvc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/kserve/api/kserve_client.py:125\u001b[0m, in \u001b[0;36mKServeClient.create\u001b[0;34m(self, inferenceservice, namespace, watch, timeout_seconds)\u001b[0m\n\u001b[1;32m    118\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_instance\u001b[38;5;241m.\u001b[39mcreate_namespaced_custom_object(\n\u001b[1;32m    119\u001b[0m         constants\u001b[38;5;241m.\u001b[39mKSERVE_GROUP,\n\u001b[1;32m    120\u001b[0m         version,\n\u001b[1;32m    121\u001b[0m         namespace,\n\u001b[1;32m    122\u001b[0m         constants\u001b[38;5;241m.\u001b[39mKSERVE_PLURAL,\n\u001b[1;32m    123\u001b[0m         inferenceservice)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m client\u001b[38;5;241m.\u001b[39mrest\u001b[38;5;241m.\u001b[39mApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException when calling CustomObjectsApi->create_namespaced_custom_object:\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124m         \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m watch:\n\u001b[1;32m    130\u001b[0m     isvc_watch(\n\u001b[1;32m    131\u001b[0m         name\u001b[38;5;241m=\u001b[39moutputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    132\u001b[0m         namespace\u001b[38;5;241m=\u001b[39mnamespace,\n\u001b[1;32m    133\u001b[0m         timeout_seconds\u001b[38;5;241m=\u001b[39mtimeout_seconds)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception when calling CustomObjectsApi->create_namespaced_custom_object:                 (409)\nReason: Conflict\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '2eb8aaf5-317c-40a4-aee3-c1f08dc4c0be', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'X-Kubernetes-Pf-Flowschema-Uid': '7de4939b-bceb-4511-b9e3-2bd606c1798f', 'X-Kubernetes-Pf-Prioritylevel-Uid': 'b71e55c5-88fe-4fee-8b3a-546ff7e59472', 'Date': 'Thu, 28 Nov 2024 13:02:59 GMT', 'Content-Length': '274'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"inferenceservices.serving.kserve.io \\\"bike-lgbm-2a\\\" already exists\",\"reason\":\"AlreadyExists\",\"details\":{\"name\":\"bike-lgbm-2a\",\"group\":\"serving.kserve.io\",\"kind\":\"inferenceservices\"},\"code\":409}\n\n\n"
     ]
    }
   ],
   "source": [
    "new_params = {\"num_leaves\": 31, \"learning_rate\": 0.01, \"random_state\": 42}\n",
    "new_model_s3_uri = train(model_type=\"lgbm\", model_params=new_params, freshness_tag=\"new\")\n",
    "\n",
    "deploy_model(model_name, new_model_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           URL                                                READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION            AGE\n",
      "bike-lgbm-2a   http://bike-lgbm-2a.kserve-inference.example.com   True           100                              bike-lgbm-2a-predictor-00001   17m\n"
     ]
    }
   ],
   "source": [
    "# Check if the updated inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'bike-lgbm-2a', 'id': '8f5caa1f-3943-47ba-8212-33bac9900853', 'parameters': {}, 'outputs': [{'name': 'output-1', 'shape': [2, 1], 'datatype': 'FP64', 'data': [51.00457318737209, 35.13687405851507]}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'bike-lgbm-2a',\n",
       " 'id': '8f5caa1f-3943-47ba-8212-33bac9900853',\n",
       " 'parameters': {},\n",
       " 'outputs': [{'name': 'output-1',\n",
       "   'shape': [2, 1],\n",
       "   'datatype': 'FP64',\n",
       "   'data': [51.00457318737209, 35.13687405851507]}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_request(model_name=model_name)\n",
    "\n",
    "# The output data should be different from the previous response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"bike-lgbm-2a\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up by removing the \"bike-lgbm-2a\" inference service\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-2a\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) Use a YAML file to deploy the model\n",
    "Instead of using the KServe SDK, now you need to use a YAML file to deploy your LightGBM model again. Please complete the configuration in [manifests/bike-lgbm-basic.yaml](./manifests/bike-lgbm-basic.yaml). You can use whichever LightGBM model in this assignment. \n",
    "\n",
    "**Hint**: You can check from [this KServe doc](https://kserve.github.io/website/0.11/modelserving/v1beta1/mlflow/v2/#deploy-with-inferenceservice) on how to use a YAML manifest to deploy a model stored in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/bike-lgbm unchanged\n"
     ]
    }
   ],
   "source": [
    "# Deploy the LightGBM model for bike demand prediction as an inference service named \"bike-lgbm\"\n",
    "!kubectl apply -f manifests/bike-lgbm-basic.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION         AGE\n",
      "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-00002   70m\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the \"bike-lgbm\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                 AGE\n",
    "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-default-00001   2m24s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                    READY   STATUS             RESTARTS      AGE\n",
      "bike-lgbm-predictor-00001-deployment-8c67c9584-8v4cg    0/2     CrashLoopBackOff   5 (52s ago)   4m58s\n",
      "bike-lgbm-predictor-00002-deployment-557755fdc8-hcf2t   0/2     Terminating        4             4m9s\n",
      "bike-lgbm-predictor-00002-deployment-557755fdc8-r8nbl   0/2     Completed          1 (8s ago)    29s\n",
      "bike-lgbm-predictor-00002-deployment-c64b4d8c6-d6wj4    0/2     CrashLoopBackOff   8 (84s ago)   18m\n"
     ]
    }
   ],
   "source": [
    "# Make sure there is one pod running for the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output: \n",
    "\n",
    "```text\n",
    "NAME                                                           READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-predictor-default-00001-deployment-9d7b87595-k9kpk   2/2     Running   0          70s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Send some requests to the \"bike-lgbm\" inference service\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbike-lgbm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/engineering_of_ml_systems/exe_week4/week4_tutorial_assignments/week4_assignments/utils/kserve_utils.py:39\u001b[0m, in \u001b[0;36msend_request\u001b[0;34m(to_ig, model_name, ig_name, model_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHost\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.kserve-inference.example.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkserve_gateway_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/v2/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/infer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 39\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# When model_type is random, the inference graph is expected to return 404 as it can't process the request\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe request was not correctly processed, got status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Send some requests to the \"bike-lgbm\" inference service\n",
    "send_request(model_name=\"bike-lgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "{'model_name': 'bike-lgbm',\n",
    " 'id': '85c9e931-0879-4f88-a84c-137063e35064',\n",
    " 'parameters': {},\n",
    " 'outputs': [{'name': 'output-1',\n",
    "   'shape': [2, 1],\n",
    "   'datatype': 'FP64',\n",
    "   'data': [51.00457318737209, 35.13687405851507]}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Please don't delete the \"bike-lgbm\" inference service, you will need it in Assignment3 later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Canary deployment in KServe (2 points)\n",
    "In this assignment, your task is to deploy the new model to KServe using the canary deployment strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to make sure there's already a \"bike-lgbm\" inference service running in KServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): inferenceservices.serving.kserve.io \"bike-lgbm\" not found\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kserve-inference get isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output: \n",
    "\n",
    "```text\n",
    "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                 AGE\n",
    "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-default-00001   47m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task is to complete the configuration in [manifests/bike-lgbm-canary.yaml](./manifests/bike-lgbm-canary.yaml) to deploy a LightGBM model using canary deployment (Please use a different LightGBM model than the one you used in Assignment 2b). Your new inference service should receive **30%** of the user traffic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/bike-lgbm configured\n"
     ]
    }
   ],
   "source": [
    "# Update the \"bike-lgbm\" inference service to use the new model\n",
    "!kubectl apply -f manifests/bike-lgbm-canary.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm configured\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION         AGE\n",
      "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-00002   94m\n"
     ]
    }
   ],
   "source": [
    "# Check that the traffic is splitted between the old and the new version\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION               LATESTREADYREVISION                 AGE\n",
    "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True    70     30       bike-lgbm-predictor-default-00001   bike-lgbm-predictor-default-00002   61m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                   READY   STATUS             RESTARTS         AGE\n",
      "bike-lgbm-predictor-00001-deployment-8c67c9584-8v4cg   0/2     CrashLoopBackOff   9 (3m46s ago)    26m\n",
      "bike-lgbm-predictor-00002-deployment-c64b4d8c6-d6wj4   0/2     CrashLoopBackOff   12 (2m24s ago)   40m\n"
     ]
    }
   ],
   "source": [
    "# Check there are two pods (one old and one new one) running \n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME                                                           READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-predictor-default-00001-deployment-cc96598f-rr2xz    2/2     Running   0          63m\n",
    "bike-lgbm-predictor-default-00002-deployment-6d9f5bbff-8mhn8   2/2     Running   0          3m36s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"bike-lgbm\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up by removing the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Horizontal autoscaling (2 points)\n",
    "\n",
    "In this assignment, your task is to complete the configuration in [manifests/bike-lgbm-scale.yaml](./manifests/bike-lgbm-scale.yaml) to deploy your LightGBM model to KServe and configure the horizontal autoscaling feature for the deployed inference service. Specifically, the horizontal autoscaling of the inference service should satisfy the following requirements:\n",
    "1. The inference service should have ae least **2** pods running;\n",
    "2. The inference service can have at most **8** pods running when it's being scaled up;\n",
    "3. The target of the auto-scaling is that each pod running the inference service should receive **5** requests per second.\n",
    "\n",
    "You can use whichever LightGBM model you trained before. \n",
    "\n",
    "**Hint**: \"rps\" should be used as the scaling metric. \n",
    "\n",
    "*rps (requests per second) VS concurrency: These two metrics may look similar at the first glance. Both of them are metrics used to measure service performance. rps quantifies the number of requests a service can process within a specific time frame, often a second, whereas concurrency focuses on how many tasks a service can handle simultaneously.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/bike-lgbm-scale created\n",
      "horizontalpodautoscaler.autoscaling/bike-lgbm-scale configured\n"
     ]
    }
   ],
   "source": [
    "# Deploy an inference service named \"bike-lgbm-scale\"\n",
    "!kubectl apply -f manifests/bike-lgbm-scale.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-scale created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              URL   READY     PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION   AGE\n",
      "bike-lgbm-scale         Unknown                                                                 22s\n"
     ]
    }
   ],
   "source": [
    "# Make sure the \"bike-lgbm-scale\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME              URL                                                   READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                       AGE\n",
    "bike-lgbm-scale   http://bike-lgbm-scale.kserve-inference.example.com   True           100                              bike-lgbm-scale-predictor-default-00001   90s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                          READY   STATUS    RESTARTS     AGE\n",
      "bike-lgbm-scale-predictor-00001-deployment-5f7f6bcfcc-qzzwj   2/2     Running   1 (2s ago)   26s\n"
     ]
    }
   ],
   "source": [
    "# Make sure there are two pods (replicas) running for the \"bike-lgbm-scale\" inference service\n",
    "# Please note that this command only check that your inference service is running, but it doesn't check if the scaling configuration is correct,\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=bike-lgbm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-scale-predictor-default-00001-deployment-66df7bcd67mr   2/2     Running   0          7m26s\n",
    "bike-lgbm-scale-predictor-default-00001-deployment-66df7bcjb6qx   2/2     Running   0          7m27s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"bike-lgbm-scale\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up by removing the \"bike-lgbm-scale\" inference service\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-scale\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Inference graph in KServe (3 points)\n",
    "\n",
    "## 5a) Inference graph for ensemble\n",
    "So far you already have two LightGBM models for predicting bike sharing demand. One was trained using the hyperparameters of {learning_rate=0.05, num_leaves=63} (denoted by Model A) and another {learning_rate=0.01, num_leaves=31} (denoted by Model B). \n",
    "\n",
    "You need to first complete the configuration in [manifests/bike-lgbm-graph.yaml](./manifests/bike-lgbm-graph.yaml) to deploy two inference services named \"bike-lgbm-1\" and \"bike-lgbm-2\". The \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services should serve Model A and B, respectively.\n",
    "\n",
    "Next, you need to complete [manifests/inference-graph1.yaml](./manifests/inference-graph1.yaml) to deploy an inference graph that includes one ensemble routing node. With this inference graph, a user will receive two predictions (one from each inference service) when they send a request.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/bike-lgbm-1 configured\n",
      "inferenceservice.serving.kserve.io/bike-lgbm-2 configured\n"
     ]
    }
   ],
   "source": [
    "# Deploy the \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services\n",
    "!kubectl apply -f manifests/bike-lgbm-graph.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-1 created\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-2 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION           AGE\n",
      "bike-lgbm-1   http://bike-lgbm-1.kserve-inference.example.com   True           100                              bike-lgbm-1-predictor-00002   11m\n",
      "bike-lgbm-2   http://bike-lgbm-2.kserve-inference.example.com   True           100                              bike-lgbm-2-predictor-00001   11m\n"
     ]
    }
   ],
   "source": [
    "# Make sure the two inference services are ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-1 bike-lgbm-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "```text\n",
    "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                   AGE\n",
    "bike-lgbm-1   http://bike-lgbm-1.kserve-inference.example.com   True           100                              bike-lgbm-1-predictor-default-00001   105m\n",
    "bike-lgbm-2   http://bike-lgbm-2.kserve-inference.example.com   True           100                              bike-lgbm-2-predictor-default-00001   105m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                      READY   STATUS                  RESTARTS        AGE\n",
      "bike-lgbm-1-predictor-00001-deployment-659476f684-c2mdk   0/2     Init:CrashLoopBackOff   6 (4m23s ago)   11m\n",
      "bike-lgbm-1-predictor-00002-deployment-99d8648ff-hg62g    2/2     Running                 0               80s\n",
      "bike-lgbm-2-predictor-00001-deployment-6448f76548-2j2vl   2/2     Running                 0               11m\n"
     ]
    }
   ],
   "source": [
    "# Make sure there are two pods running for the two inference services, respectively\n",
    "!kubectl -n kserve-inference get pods -l \"serving.kserve.io/inferenceservice in (bike-lgbm-1,bike-lgbm-2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-1-predictor-default-00001-deployment-794547df56-48dhk   2/2     Running   0          109m\n",
    "bike-lgbm-2-predictor-default-00001-deployment-cf7b449b5-rjc2q    2/2     Running   0          109m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencegraph.serving.kserve.io/my-graph1 created\n"
     ]
    }
   ],
   "source": [
    "# Deploy the inference graph named \"my-graph1\"\n",
    "!kubectl apply -f manifests/inference-graph1.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferencegraph.serving.kserve.io/my-graph1 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        URL                                             READY   AGE\n",
      "my-graph1   http://my-graph1.kserve-inference.example.com   True    19s\n"
     ]
    }
   ],
   "source": [
    "# Make sure the \"my-graph1\" inference graph is ready\n",
    "!kubectl -n kserve-inference get ig my-graph1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "```text\n",
    "NAME        URL                                             READY   AGE\n",
    "my-graph1   http://my-graph1.kserve-inference.example.com   True    102s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          READY   STATUS    RESTARTS   AGE\n",
      "my-graph1-00001-deployment-86846555b9-w7dzf   2/2     Running   0          20s\n"
     ]
    }
   ],
   "source": [
    "# Also make sure there is one pod running for the \"ensemble\" inference graph\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferencegraph=my-graph1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "```text\n",
    "NAME                                          READY   STATUS    RESTARTS   AGE\n",
    "my-graph1-00001-deployment-7c4d7cfbf9-tr5tz   2/2     Running   0          2m7s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's send a request to the \"my-graph1\" inference graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The request was not correctly processed, got status code 404",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Send a request\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkserve_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m send_request\n\u001b[0;32m----> 4\u001b[0m \u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_ig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy-graph1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/engineering_of_ml_systems/exe_week4/week4_tutorial_assignments/week4_assignments/utils/kserve_utils.py:42\u001b[0m, in \u001b[0;36msend_request\u001b[0;34m(to_ig, model_name, ig_name, model_type)\u001b[0m\n\u001b[1;32m     39\u001b[0m result \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, json\u001b[38;5;241m=\u001b[39mrequest_data, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# When model_type is random, the inference graph is expected to return 404 as it can't process the request\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe request was not correctly processed, got status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mAssertionError\u001b[0m: The request was not correctly processed, got status code 404"
     ]
    }
   ],
   "source": [
    "# Send a request\n",
    "from utils.kserve_utils import send_request\n",
    "\n",
    "send_request(to_ig=True, ig_name=\"my-graph1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output (i.e., the response) is expected to contain a prediction from the \"bike-lgbm-1\" inference service and another prediction from the \"bike-lgbm-2\" inference service. \n",
    "\n",
    "Example output:\n",
    "```text\n",
    "{'bike-lgbm-v1': {'id': '3f16b921-e18a-4b51-93c3-f084bf11d07c',\n",
    "  'model_name': 'bike-lgbm-1',\n",
    "  'outputs': [{'data': [51.00457318737209, 35.13687405851507],\n",
    "    'datatype': 'FP64',\n",
    "    'name': 'output-1',\n",
    "    'shape': [2, 1]}],\n",
    "  'parameters': {}},\n",
    " 'bike-lgbm-v2': {'id': 'ef1c1ec3-9880-467d-8773-1bf614d8e0bf',\n",
    "  'model_name': 'bike-lgbm-2',\n",
    "  'outputs': [{'data': [97.60649891558708, 94.67018085698945],\n",
    "    'datatype': 'FP64',\n",
    "    'name': 'output-1',\n",
    "    'shape': [2, 1]}],\n",
    "  'parameters': {}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Do not delete the \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services. They're still needed in the next assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b) More complicated inference graph\n",
    "In this assignment, you need to deploy a more complex inference graph containing more than one routing node. \n",
    "\n",
    "First let's train two XGBoost models, denoted by Model C and Model D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model found, start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/28 22:37:22 INFO mlflow.tracking.fluent: Experiment with name 'week4-xgb-bike-demand' does not exist. Creating a new experiment.\n",
      "/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:37:23] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "INFO:botocore.credentials:Found credentials in environment variables.\n",
      "Successfully registered model 'Week4XgbBikeDemand'.\n",
      "2024/11/28 22:37:32 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Week4XgbBikeDemand, version 1\n",
      "Created version '1' of model 'Week4XgbBikeDemand'.\n",
      "/Users/liupei/Desktop/engineering_of_ml_systems/exe_week4/week4_tutorial_assignments/week4_assignments/utils/common_utils.py:135: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.9.2/model-registry.html#migrating-from-stages\n",
      "  model_info = MLFLOW_CLIENT.get_latest_versions(registered_model_name)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained model is located at s3://mlflow/8/4506ddda675f4c3c997d22bae1f4c03e/artifacts/xgb-bike\n",
      "No model found, start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [22:37:33] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/_distutils_hack/__init__.py:11: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Registered model 'Week4XgbBikeDemand' already exists. Creating a new version of this model...\n",
      "2024/11/28 22:37:35 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Week4XgbBikeDemand, version 2\n",
      "Created version '2' of model 'Week4XgbBikeDemand'.\n",
      "/Users/liupei/Desktop/engineering_of_ml_systems/exe_week4/week4_tutorial_assignments/week4_assignments/utils/common_utils.py:135: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.9.2/model-registry.html#migrating-from-stages\n",
      "  model_info = MLFLOW_CLIENT.get_latest_versions(registered_model_name)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained model is located at s3://mlflow/8/a3de4770e6704e9cbeb89d7e900737dd/artifacts/xgb-bike\n",
      "First xgb model URI: s3://mlflow/8/4506ddda675f4c3c997d22bae1f4c03e/artifacts/xgb-bike\n",
      "Second xgb model URI: s3://mlflow/8/a3de4770e6704e9cbeb89d7e900737dd/artifacts/xgb-bike\n"
     ]
    }
   ],
   "source": [
    "old_xgb_model_s3_uri = train(\n",
    "    model_type=\"xgb\",\n",
    "    model_params={\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"objective\": \"reg:absoluteerror\",\n",
    "        \"random_state\": 42,\n",
    "    },\n",
    "    freshness_tag=\"old\",\n",
    ")\n",
    "\n",
    "new_xgb_model_s3_uri = train(\n",
    "    model_type=\"xgb\",\n",
    "    model_params={\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"objective\": \"reg:absoluteerror\",\n",
    "        \"random_state\": 42,\n",
    "    },\n",
    "    freshness_tag=\"new\",\n",
    ")\n",
    "\n",
    "print(\"First xgb model URI:\", old_xgb_model_s3_uri)\n",
    "print(\"Second xgb model URI:\", new_xgb_model_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Assignment 5a, your tasks are to\n",
    "1. Complete [manifests/bike-xgb-graph.yaml](./manifests/bike-xgb-graph.yaml) to deploy two more inference services named \"bike-xgb-1\" and \"bike-xgb-2\" that serve Models C and D, respectively. \n",
    "2. Complete [manifests/inference-graph2.yaml](./manifests/inference-graph2.yaml) to deploy an inference graph containing two Ensemble routing nodes. \n",
    "The requests that will be sent to the inference graph look like:\n",
    "    ```python\n",
    "    {\n",
    "      'inputs': ...,\n",
    "      'modelType': 'lgbm'\n",
    "    }\n",
    "    ```\n",
    "    The inference graph should satisfy the following requirements:\n",
    "    - If there is a field named \"modelType\" in the request and its value is \"lgbm\", the request should be forwarded to an ensemble consisting of the \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services (These two inference services should be created in Assignment 5a). At this time, the user should receive one prediction from the \"bike-lgbm-1\" inference service and another prediction from the \"bike-lgbm-2\" inference service. \n",
    "    - If the value of \"modelType is \"xgb\", the request should be forwarded to another ensemble consisting of the \"bike-xgb-1\" and \"bike-xgb-2\" inference services. At this time, the user should receive one prediction from the \"bike-xgb-1\" inference service and another prediction from the \"bike-xgb-2\" inference service.\n",
    "    - Otherwise an error message should be returned, complaining that the request can't be processed. \n",
    "    \n",
    "    The behavior of the inference graph is illustrated in the figure below:\n",
    "\n",
    "    <img src=\"./images/complex-inference-graph.png\" width=600/>\n",
    "\n",
    "**Hints**:\n",
    "You may notice that you need to route requests from one routing node to another (instead of from a routing node to an inference service). Below is an example of configuring a routing node to forward requests to another routing node:\n",
    "```yaml\n",
    "...\n",
    "spec: \n",
    "  nodes: \n",
    "    # The first routing node\n",
    "    root: \n",
    "      routerType: ...\n",
    "      steps: \n",
    "      # This routing node forwards requests to the second routing node named \"ensembleNode\"\n",
    "      - nodeName: ensembleNode\n",
    "\n",
    "    # The second routing node\n",
    "    ensembleNode:\n",
    "      routerType: ...\n",
    "      steps:\n",
    "      ...\n",
    "```\n",
    "You can use `\"[@this].#(modelType==\\\"...\\\")\"` as the condition that determines which ensemble a request should be routed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/bike-xgb-1 created\n",
      "inferenceservice.serving.kserve.io/bike-xgb-2 created\n"
     ]
    }
   ],
   "source": [
    "# Deploy the third inference service named \"bike-lgbm-3\"\n",
    "!kubectl apply -f manifests/bike-xgb-graph.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-xgb-1 created\n",
    "inferenceservice.serving.kserve.io/bike-xgb-2 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION           AGE\n",
      "bike-lgbm-1   http://bike-lgbm-1.kserve-inference.example.com   True           100                              bike-lgbm-1-predictor-00002   82m\n",
      "bike-lgbm-2   http://bike-lgbm-2.kserve-inference.example.com   True           100                              bike-lgbm-2-predictor-00001   82m\n",
      "bike-xgb-1    http://bike-xgb-1.kserve-inference.example.com    True           100                              bike-xgb-1-predictor-00001    2m4s\n",
      "bike-xgb-2    http://bike-xgb-2.kserve-inference.example.com    True           100                              bike-xgb-2-predictor-00001    2m4s\n"
     ]
    }
   ],
   "source": [
    "# Make sure the all of the four inference services are ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-1 bike-lgbm-2 bike-xgb-1 bike-xgb-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION           AGE\n",
    "bike-lgbm-1   http://bike-lgbm-1.kserve-inference.example.com   True           100                              bike-lgbm-1-predictor-00001   9m21s\n",
    "bike-lgbm-2   http://bike-lgbm-2.kserve-inference.example.com   True           100                              bike-lgbm-2-predictor-00001   9m20s\n",
    "bike-xgb-1    http://bike-xgb-1.kserve-inference.example.com    True           100                              bike-xgb-1-predictor-00001    56s\n",
    "bike-xgb-2    http://bike-xgb-2.kserve-inference.example.com    True           100                              bike-xgb-2-predictor-00001    56s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                      READY   STATUS    RESTARTS   AGE\n",
      "bike-lgbm-1-predictor-00002-deployment-99d8648ff-hg62g    2/2     Running   0          71m\n",
      "bike-lgbm-2-predictor-00001-deployment-6448f76548-2j2vl   2/2     Running   0          82m\n",
      "bike-xgb-1-predictor-00001-deployment-66559f8c57-h4wg7    2/2     Running   0          2m8s\n",
      "bike-xgb-2-predictor-00001-deployment-6ffb7b49c4-whjzj    2/2     Running   0          2m7s\n"
     ]
    }
   ],
   "source": [
    "# Make sure there are three pods running for the three inference services, respectively\n",
    "!kubectl -n kserve-inference get pods -l \"serving.kserve.io/inferenceservice in (bike-lgbm-1,bike-lgbm-2,bike-xgb-1, bike-xgb-2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME                                                      READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-1-predictor-00001-deployment-68668c6667-5rb89   2/2     Running   0          9m48s\n",
    "bike-lgbm-2-predictor-00001-deployment-7d69db959f-bl75c   2/2     Running   0          9m47s\n",
    "bike-xgb-1-predictor-00001-deployment-5f7fdffbf8-l5q2p    2/2     Running   0          83s\n",
    "bike-xgb-2-predictor-00001-deployment-7797598c87-sxn49    2/2     Running   0          83s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencegraph.serving.kserve.io/my-graph2 created\n"
     ]
    }
   ],
   "source": [
    "# Deploy the second inference graph (\"my-graph2\")\n",
    "!kubectl apply -f manifests/inference-graph2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferencegraph.serving.kserve.io/my-graph2 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        URL                                             READY   AGE\n",
      "my-graph2   http://my-graph2.kserve-inference.example.com   True    16s\n"
     ]
    }
   ],
   "source": [
    "# Make sure the inference graph named \"my-graph2\" is ready\n",
    "!kubectl -n kserve-inference get ig my-graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME        URL                                             READY   AGE\n",
    "my-graph2   http://my-graph2.kserve-inference.example.com   True    35s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          READY   STATUS    RESTARTS   AGE\n",
      "my-graph2-00001-deployment-77df99c7db-wwgpd   2/2     Running   0          19s\n"
     ]
    }
   ],
   "source": [
    "# Also make sure there is one pod running for the \"ensemble\" inference graph\n",
    "!kubectl -n kserve-inference get pods -l  serving.kserve.io/inferencegraph=my-graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME                                          READY   STATUS    RESTARTS   AGE\n",
    "my-graph2-00001-deployment-557678dfbd-zglcg   2/2     Running   0          49s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The request was not correctly processed, got status code 500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Send some requests\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_ig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy-graph2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlgbm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/engineering_of_ml_systems/exe_week4/week4_tutorial_assignments/week4_assignments/utils/kserve_utils.py:42\u001b[0m, in \u001b[0;36msend_request\u001b[0;34m(to_ig, model_name, ig_name, model_type)\u001b[0m\n\u001b[1;32m     39\u001b[0m result \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, json\u001b[38;5;241m=\u001b[39mrequest_data, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# When model_type is random, the inference graph is expected to return 404 as it can't process the request\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe request was not correctly processed, got status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mAssertionError\u001b[0m: The request was not correctly processed, got status code 500"
     ]
    }
   ],
   "source": [
    "# Send some requests\n",
    "send_request(to_ig=True, ig_name=\"my-graph2\", model_type=\"lgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is expected to contain predictions from both \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```text\n",
    "{'bike-lgbm-v1': {'id': '09be4efc-65f4-40ce-be91-e326bcf74ca5',\n",
    "  'model_name': 'bike-lgbm-1',\n",
    "  'outputs': [{'data': [51.00457318737209, 35.13687405851507],\n",
    "    'datatype': 'FP64',\n",
    "    'name': 'output-1',\n",
    "    'shape': [2, 1]}],\n",
    "  'parameters': {}},\n",
    " 'bike-lgbm-v2': {'id': 'd3c53425-e426-4a79-8517-477994a7c49b',\n",
    "  'model_name': 'bike-lgbm-2',\n",
    "  'outputs': [{'data': [97.60649891558708, 94.67018085698945],\n",
    "    'datatype': 'FP64',\n",
    "    'name': 'output-1',\n",
    "    'shape': [2, 1]}],\n",
    "  'parameters': {}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The request was not correctly processed, got status code 500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_ig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy-graph2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxgb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/engineering_of_ml_systems/exe_week4/week4_tutorial_assignments/week4_assignments/utils/kserve_utils.py:42\u001b[0m, in \u001b[0;36msend_request\u001b[0;34m(to_ig, model_name, ig_name, model_type)\u001b[0m\n\u001b[1;32m     39\u001b[0m result \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, json\u001b[38;5;241m=\u001b[39mrequest_data, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# When model_type is random, the inference graph is expected to return 404 as it can't process the request\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe request was not correctly processed, got status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mAssertionError\u001b[0m: The request was not correctly processed, got status code 500"
     ]
    }
   ],
   "source": [
    "send_request(to_ig=True, ig_name=\"my-graph2\", model_type=\"xgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response should only have predictions from the \"bike-xgb-1\" and \"bike-xgb-2\" inference services.\n",
    "\n",
    "Example output:\n",
    "```text\n",
    "{'bike-xgb-v1': {'id': '51e997c0-74e2-49b7-a273-3c489c848486',\n",
    "  'model_name': 'bike-xgb-1',\n",
    "  'outputs': [{'data': [112.01314544677734, 97.72420501708984],\n",
    "    'datatype': 'FP32',\n",
    "    'name': 'predict',\n",
    "    'shape': [2, 1]}],\n",
    "  'parameters': {}},\n",
    " 'bike-xgb-v2': {'id': '71d65beb-c085-4787-8151-2a3f0fe6f504',\n",
    "  'model_name': 'bike-xgb-2',\n",
    "  'outputs': [{'data': [137.37649536132812, 131.63937377929688],\n",
    "    'datatype': 'FP32',\n",
    "    'name': 'predict',\n",
    "    'shape': [2, 1]}],\n",
    "  'parameters': {}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Failed to process request', 'cause': 'invalid route type: '}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': 'Failed to process request', 'cause': 'invalid route type: '}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_request(to_ig=True, ig_name=\"my-graph2\", model_type=\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error message should be returned.\n",
    "\n",
    "Expected output:\n",
    "\n",
    "```text\n",
    "{'error': 'Failed to process request',\n",
    " 'cause': 'None of the routes matched with the switch condition'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"bike-lgbm-1\" deleted\n",
      "inferenceservice.serving.kserve.io \"bike-lgbm-2\" deleted\n",
      "inferenceservice.serving.kserve.io \"bike-xgb-1\" deleted\n",
      "inferenceservice.serving.kserve.io \"bike-xgb-2\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete all of the three inference services\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm-1 bike-lgbm-2 bike-xgb-1 bike-xgb-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-1\" deleted\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-2\" deleted\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-3\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencegraph.serving.kserve.io \"my-graph1\" deleted\n",
      "inferencegraph.serving.kserve.io \"my-graph2\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete all inference graphs\n",
    "!kubectl -n kserve-inference delete ig my-graph1 my-graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferencegraph.serving.kserve.io \"my-graph1\" deleted\n",
    "inferencegraph.serving.kserve.io \"my-graph2\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-up\n",
    "Please make sure you have the following files in your submission:\n",
    "- part1_answer.py and part2_answer.py\n",
    "- model-settings.json \n",
    "- all YAML files listed in the \"manifests\" directory\n",
    "\n",
    "When submit the files, please **do not** change the file names or put any of them in any sub-folder. The screenshot below shows an expected submission:\n",
    "\n",
    "<img src=\"./images/submission-example.png\" width=700/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_eng2_t1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
