{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week4 Assignments\n",
    "**Please use the \"mlops_eng2\" conda environment to complete all assignments for this week.**\n",
    "\n",
    "In this week's assignments, you'll gain more hands-on experience deploying ML models, especially using KServe. The assignments are split into two separate notebooks. This notebook is the first part containing Assignment 1. The second part containing Assignments 2-5 can be found in `week4_assignments_part2.ipynb`. \n",
    "\n",
    "### Guidelines for submitting the assignments\n",
    "For some assignments, a code skeleton is provided. Please put your solutions between the `### START CODE HERE` and `### END CODE HERE` code comments. Please **do not change any code other than those between the `### START CODE HERE` and `### END CODE HERE` comments**. Unlike the previous weeks, you don't need to return the \".ipynb\" notebooks as your code written in the notebooks will be exported to Python scripts. The notebooks contain the instructions and some code that help you check if you're progressing correctly. Please return the following files in your submission: \n",
    "- The Python scripts (`part1_answer.py` and `part2_answer.py`, these files will be created when you progress with the assignments).\n",
    "- `model-settings.json`\n",
    "- All the `.yaml` files in the \"manifests\" directory.\n",
    "\n",
    "***Important!*** When submitting the files, please **do not** change the file names or put any of them in any sub-folder. The screenshot below shows an expected submission:\n",
    "\n",
    "<img src=\"./images/submission-example.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Use MLServer to deploy a model locally (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from mlserver.codecs import PandasCodec\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "from utils.mlserver_utils import prepare_request_data, run_mlserver\n",
    "from utils.common_utils import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In previous weekly assignments, we have used LightGBM version 4.0.0, which is not compatible with the default runtime provided by KServe\n",
    "# Though we don't need KServe in this particular assignment, to avoid switching between different versions of LightGBM models, \n",
    "# we are using LightGBM version 3.3.5 in all assignments of this week. \n",
    "import lightgbm\n",
    "assert lightgbm.__version__ == \"3.3.5\", \"Your lightgbm version is not 3.3.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "Let's first train a LightGBM regression model for predicting the bike sharing demand (the use case in Week1 assignments) and upload the model to the MLflow service. After running the next code cell, you should see the S3 URI of your model printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found, skip training and use the existing model s3://mlflow/7/c6593f7cd39f4444acd8581588de91af/artifacts/lgbm-bike\n",
      "Your model S3 URI is s3://mlflow/7/c6593f7cd39f4444acd8581588de91af/artifacts/lgbm-bike\n"
     ]
    }
   ],
   "source": [
    "params = {\"num_leaves\": 63, \"learning_rate\": 0.05, \"random_state\": 42}\n",
    "\n",
    "model_s3_uri = train(model_type=\"lgbm\", model_params=params, freshness_tag=\"old\")\n",
    "\n",
    "print(f\"Your model S3 URI is {model_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment1 instructions\n",
    "\n",
    "Then let's take a look at what MLServer is. Shortly speaking, [MLServer](https://mlserver.readthedocs.io/en/latest/index.html) is an open-source inference server implementation for ML models. It provides an easy way to expose a model through an HTTP or gRPC endpoint. Reading the following MLServer documentation should be enough to complete the assignment:\n",
    "- [Getting started with MLServer](https://mlserver.readthedocs.io/en/latest/getting-started/index.html#). You'll see an example of using MLServer SDK to implement a custom model server in this documentation. You don't need to implement your own model server to complete this assignment as MLServer has an out-of-box inference server implementation for models registered to MLflow (see the second documentation). \n",
    "- [Serving MLflow models](https://mlserver.readthedocs.io/en/latest/examples/mlflow/README.html).\n",
    "\n",
    "You already trained a LightGBM model for predicting bike sharing demand and upload it to the MLflow service by running the previous code cell. In this assignment, you need to configure MLServer to serve your LightGBM model as an inference service locally. Specifically, you have two tasks:\n",
    "\n",
    "1. Add configurations to the empty [model-settings.json](./assignment1/model-settings.json)(The file will be created later) to use the MLServer's MLflow runtime to serve your LightGBM model. The inference service name should be ***bike-demand-predictor***. The configuration can be adapted from the [one provided by this MLServer doc.](https://mlserver.readthedocs.io/en/latest/examples/mlflow/README.html#serving)\n",
    "1. Now suppose you are in the same directory where this notebook is located, what command should be used to start an MLServer inference service to serve the LightGBM model? Please assign your command as a string to the `command_to_start_mlserver` variable in a code cell below.  \n",
    "\n",
    "**Notes**:\n",
    "You might want to first test your command in a terminal, please note that MLServer will load the model from your MinIO storage service so you need to specify the following environment variables to allow MLServer to use the correct credentials to load the model from the correct MinIO service endpoint:\n",
    "```bash\n",
    "# Run the following command in a terminal\n",
    "export AWS_ACCESS_KEY_ID=minioadmin\n",
    "export AWS_SECRET_ACCESS_KEY=minioadmin\n",
    "export MLFLOW_S3_ENDPOINT_URL=http://mlflow-minio.local\n",
    "```\n",
    "These environment variables are only available in the terminal session where you defined them, so you need to start your MLServer inference service in the same terminal session where you defined the above environment variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model-settings.json file\n",
    "open(\"model-settings.json\", \"a\").close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose you are in the same directory where this notebook is located, what command should be used to start an MLServer inference service to serve the LightGBM model? Please assign your command as a string to the `command_to_start_mlserver` variable in the code cell below.\n",
    "\n",
    "After completing and running the next code cell, you should see a new file `part1_answer.py` created. This Python file should contain the code you write in the next code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ee45bae0078532b69128ed91e652ca2",
     "grade": false,
     "grade_id": "cell-1cb66dbda47ba8f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting part1_answer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile part1_answer.py\n",
    "# TODO: Put your command to start mlserver in the variable below\n",
    "# command_to_start_mlserver = \"your command as a string here\"\n",
    "### START CODE HERE\n",
    "command_to_start_mlserver = 'mlserver start ./week4_assignments/model-settings.json'\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'parameters': {'content_type': 'pd'}, 'inputs': [{'name': 'season', 'shape': [5, 1], 'datatype': 'INT64', 'data': [4, 4, 4, 4, 4]}, {'name': 'holiday', 'shape': [5, 1], 'datatype': 'INT64', 'data': [0, 0, 0, 0, 0]}, {'name': 'workingday', 'shape': [5, 1], 'datatype': 'INT64', 'data': [1, 1, 1, 1, 1]}, {'name': 'weather', 'shape': [5, 1], 'datatype': 'INT64', 'data': [2, 2, 2, 2, 2]}, {'name': 'temp', 'shape': [5, 1], 'datatype': 'FP64', 'data': [11.48, 11.48, 10.66, 10.66, 10.66]}, {'name': 'atemp', 'shape': [5, 1], 'datatype': 'FP64', 'data': [13.635, 12.88, 12.12, 12.12, 12.88]}, {'name': 'humidity', 'shape': [5, 1], 'datatype': 'INT64', 'data': [52, 52, 56, 56, 56]}, {'name': 'windspeed', 'shape': [5, 1], 'datatype': 'FP64', 'data': [15.0013, 19.0012, 16.9979, 19.0012, 12.998]}, {'name': 'hour', 'shape': [5, 1], 'datatype': 'INT32', 'data': [0, 1, 2, 3, 4]}, {'name': 'day', 'shape': [5, 1], 'datatype': 'INT32', 'data': [13, 13, 13, 13, 13]}, {'name': 'month', 'shape': [5, 1], 'datatype': 'INT32', 'data': [12, 12, 12, 12, 12]}]}\n"
     ]
    }
   ],
   "source": [
    "# Prepare the request data following the V2 inference protocol\n",
    "encoded_request_data = prepare_request_data()\n",
    "print(encoded_request_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the code cell below to check if your configuration and command are correct. If everything is OK, you should see some predictions returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-28 15:58:45,243 [mlserver.parallel] DEBUG - Starting response processing loop...\n",
      "2024-11-28 15:58:45,244 [mlserver.rest] INFO - HTTP server running on http://0.0.0.0:8080\n",
      "2024-11-28 15:58:45,264 [mlserver.metrics] INFO - Metrics server running on http://0.0.0.0:8082\n",
      "2024-11-28 15:58:45,264 [mlserver.metrics] INFO - Prometheus scraping endpoint can be accessed on http://0.0.0.0:8082/metrics\n",
      "2024-11-28 15:58:45,265 [mlserver.grpc] INFO - gRPC server running on http://0.0.0.0:8081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [5889]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Started server process [5889]\n",
      "INFO:     Waiting for application startup.\n",
      "/opt/anaconda3/envs/mlops_eng2_t1/lib/python3.10/site-packages/starlette_exporter/middleware.py:97: FutureWarning: group_paths and filter_unhandled_paths will change defaults from False to True in the next release. See https://github.com/stephenhillier/starlette_exporter/issues/79 for more info\n",
      "  warnings.warn(\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8082 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65407 - \"POST /v2/models/bike-demand-predictor/infer HTTP/1.1\" 404 Not Found\n",
      "2024-11-28 15:58:59,456 [mlserver.parallel] INFO - Waiting for shutdown of default inference pool...\n",
      "2024-11-28 15:58:59,554 [mlserver.parallel] INFO - Shutdown of default inference pool complete\n",
      "2024-11-28 15:58:59,554 [mlserver.grpc] INFO - Waiting for gRPC server shutdown\n",
      "2024-11-28 15:58:59,554 [mlserver.grpc] INFO - gRPC server shutdown complete\n",
      "\n",
      "Response:\n",
      "{'error': 'Model bike-demand-predictor not found'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [5889]\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [5889]\n"
     ]
    }
   ],
   "source": [
    "from part1_answer import command_to_start_mlserver\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://mlflow-minio.local\"\n",
    "\n",
    "response = run_mlserver(command_to_start_mlserver, encoded_request_data)\n",
    "print()\n",
    "print(\"Response:\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "```text\n",
    "{'model_name': 'bike-demand-predictor',\n",
    " 'id': 'f021577e-16fb-4686-8f1e-70f3ae2a7b76',\n",
    " 'parameters': {'content_type': 'np'},\n",
    " 'outputs': [{'name': 'output-1',\n",
    "   'shape': [5, 1],\n",
    "   'datatype': 'FP64',\n",
    "   'parameters': {'content_type': 'np'},\n",
    "   'data': [37.289116222680455, \n",
    "   19.406971833185164, \n",
    "   10.248384070712056, \n",
    "   9.602077884278172, \n",
    "   9.602077884278172]]}]}\n",
    "```\n",
    "The id may vary. The key point is that the response should follow the same format as the example output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can go to the [second part](./week4_assignments_part2.ipynb) of this week's assignments. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
