# PIPELINE DEFINITION
# Name: deploy-model
# Description: Args:
#              model_name: the name of the deployed inference service
#              storage_uri: the URI of the saved model in MLflow's artifact store
# Inputs:
#    model_name: str
#    storage_uri: str
components:
  comp-deploy-model:
    executorLabel: exec-deploy-model
    inputDefinitions:
      parameters:
        model_name:
          parameterType: STRING
        storage_uri:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-deploy-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - deploy_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kserve==0.11.2'\
          \ 'kfp==2.0.1' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport pandas as pd\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef deploy_model(model_name: str, storage_uri: str):\n    \"\"\"\n\
          \    Args:\n        model_name: the name of the deployed inference service\n\
          \        storage_uri: the URI of the saved model in MLflow's artifact store\n\
          \    \"\"\"\n    from kubernetes import client\n    from kserve import KServeClient\n\
          \    from kserve import constants\n    from kserve import V1beta1InferenceService\n\
          \    from kserve import V1beta1InferenceServiceSpec\n    from kserve import\
          \ V1beta1PredictorSpec\n    from kserve import V1beta1ModelSpec\n    from\
          \ kserve import V1beta1ModelFormat\n    import logging\n\n    logging.basicConfig(level=logging.INFO)\n\
          \    logger = logging.getLogger(__name__)\n\n    namespace = \"kserve-inference\"\
          \n    service_account_name = \"kserve-sa\"\n    api_version = constants.KSERVE_V1BETA1\n\
          \    logger.info(f\"MODEL URI: {storage_uri}\")\n\n    modelspec = V1beta1ModelSpec(\n\
          \        storage_uri=storage_uri,\n        model_format=V1beta1ModelFormat(name=\"\
          mlflow\"),\n        protocol_version=\"v2\"\n    )\n\n    ### START CODE\
          \ HERE\n    # Define the inference service specification\n    isvc = V1beta1InferenceService(\n\
          \        api_version=api_version,\n        kind=constants.KSERVE_KIND,\n\
          \        metadata=client.V1ObjectMeta(\n            name=model_name,\n \
          \           namespace=namespace\n        ),\n        spec=V1beta1InferenceServiceSpec(\n\
          \            predictor=V1beta1PredictorSpec(\n                model=modelspec,\n\
          \                service_account_name=service_account_name\n           \
          \ )\n        )\n    )\n\n    # Create the KServe client\n    kserve_client\
          \ = KServeClient(config_file=\"~/.kube/config\")\n\n    try:\n        #\
          \ Delete the existing inference service if it exists\n        try:\n   \
          \         kserve_client.delete(model_name, namespace=namespace)\n      \
          \      logger.info(f\"Deleted existing inference service '{model_name}'.\"\
          )\n            time.sleep(100)\n        except Exception as delete_err:\n\
          \            logger.warning(f\"Failed to delete existing inference service:\
          \ {delete_err}\")\n\n        # Create a new inference service\n        kserve_client.create(isvc)\n\
          \        logger.info(f\"Inference service '{model_name}' creation initiated.\"\
          )\n\n        # Wait for the inference service to become ready\n        kserve_client.wait_isvc_ready(model_name,\
          \ namespace=namespace)\n        logger.info(f\"Inference service '{model_name}'\
          \ is ready.\")\n\n    except Exception as e:\n        logger.error(f\"Failed\
          \ to create inference service: {e}\")\n        raise\n    ### END CODE HERE\n\
          \n"
        image: python:3.11
pipelineInfo:
  name: deploy-model
root:
  dag:
    tasks:
      deploy-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-deploy-model
        inputs:
          parameters:
            model_name:
              componentInputParameter: model_name
            storage_uri:
              componentInputParameter: storage_uri
        taskInfo:
          name: deploy-model
  inputDefinitions:
    parameters:
      model_name:
        parameterType: STRING
      storage_uri:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.1
